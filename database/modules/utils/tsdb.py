import os
import re
import glob
import sqlite3
import hashlib
import psycopg2
import pandas as pd
import numpy as np
import time
import inspect
from functools import wraps
from astropy.time import Time
from astropy.table import Table
from astropy.io import fits
from tqdm import tqdm
from tqdm.notebook import tqdm_notebook
from datetime import datetime
from functools import partial
from concurrent.futures import ProcessPoolExecutor

from kpfpipe.models.level1 import KPF1
from kpfpipe.logger import start_logger
from modules.Utils.utils import DummyLogger
from modules.Utils.kpf_parse import get_datecode

DEFAULT_CFG_PATH = 'database/modules/utils/tsdb.cfg'
        
def safe_float(value):
    try:
        return float(value)
    except (ValueError, TypeError):
        return None  # or np.nan

class TSDB:
    """
    Description:
        The TSDB (Time Series DataBase) class facilitates management, ingestion, querying, and reporting
        of time series data associated with Keck Planet Finder (KPF) observations. It supports interactions
        with both SQLite3 and PostgreSQL backends, providing flexibility for local or server-based deployments.
    
        TSDB is designed to handle diverse data products generated by KPF, including primary header keywords
        and telemetry from L0 (raw), 2D (two-dimensional calibrated), L1 (one-dimensional extracted), and L2
        (radial velocity processed) data levels. It extracts header information, telemetry values, radial
        velocity (RV) measurements, and cross-correlation function (CCF) data, storing them efficiently
        in a structured relational database schema.
    
        Core functionalities include:
        - Database initialization and schema management
        - Dynamic ingestion of observational metadata and telemetry from FITS files
        - Efficient batch processing with multiprocessing support
        - Robust data querying capabilities via user-friendly methods
        - Structured metadata handling using configurable CSV mappings
    
        The database schema and metadata mappings are configured via external CSV files, enabling
        flexible adjustments without code modifications. This facilitates easy expansion and adaptation
        to evolving data requirements and observational products.
    
    Arguments:
        backend (str):
            Specifies the type of database backend to use ('sqlite' for local file-based storage or 'psql' for PostgreSQL).
        db_path (str):
            Path to the SQLite3 database file. Ignored if backend is PostgreSQL.
        base_dir (str):
            Base directory containing KPF Level 0 (L0) observational data directories.
        logger (logging.Logger or None):
            Logger object for capturing messages, warnings, and errors. If None, a DummyLogger
            with formatted print statements is used.
        verbose (bool):
            Controls verbosity level of logging output.
    
    Attributes:
        backend (str):
            Database backend in use ('sqlite' or 'psql').
        db_path (str):
            Filesystem path to the SQLite database.
        base_dir (str):
            Root directory for observational data storage.
        tables (dict):
            Dictionary mapping database tables to their CSV configurations.
        extraction_plan (dict):
            Structured plan detailing the file level and FITS extension from which to extract
            keywords for each database table.
        metadata_entries (list):
            Aggregated list of metadata entries extracted from keyword CSV files.
        metadata_rows (list):
            Cached list of metadata entries loaded directly from the database metadata table.
        bool_columns (set):
            Set of database columns identified as boolean, ensuring proper data typing.
    
    Related Command-Line Scripts:
        - 'ingest_dates_kpf_tsdb.py':
            Ingest observational data for specified date ranges into the TSDB database.
        - 'ingest_watch_kpf_tsdb.py':
            Continuously watch directories for new observational data and automatically ingest them.
    
    To-do:
        - Implement dynamic privilege checks for PostgreSQL backend to avoid unauthorized operations.
        - Include derived temperature derivatives and other computed columns.
        - Introduce separate database management for calibration master files.
        - Provide quality control (QC) filtering options in query methods (e.g., qc_pass, qc_fail flags).
    
    """
    def __init__(self, backend='sqlite', db_path='kpf_ts.db', base_dir='/data/L0', logger=None, verbose=False):

        self.logger = logger if logger is not None else DummyLogger()
        self.logger.info('Starting KPF_TSDB')
        self.backend = backend # sqlite or psql
        if self.backend != 'sqlite' and self.backend != 'psql':
            self.logger.info("Invalid entry for backend.  Must be 'sqlite' or 'psql'.")
            return
        self.verbose = verbose
        self.base_dir = base_dir
        self.logger.info('Base data directory: ' + self.base_dir)
        self.conn = None
        self.cursor = None

        if self.is_notebook():
            self.tqdm = tqdm_notebook
            self.logger.info('Jupyter Notebook environment detected.')
        else:
            self.tqdm = tqdm

        if self.backend == 'sqlite':
            self.db_path = db_path
            self.logger.info('Path of database file: ' + os.path.abspath(self.db_path))
        elif backend == 'psql':
            self.dbport   = os.getenv('TSDBPORT')
            self.dbname   = os.getenv('TSDBNAME')
            self.dbuser   = os.getenv('TSDBUSER')
            self.dbpass   = os.getenv('TSDBPASS')
            self.dbserver = os.getenv('TSDBSERVER')     
            self.logger.info('PSQL server: ' + self.dbserver)
            self.logger.info('PSQL user: ' + self.dbuser)
            self.user_role = self.get_user_role()
            self.logger.info('PSQL user role: ' + self.user_role)
        
        # Read table information from file
        self.keyword_base_path = '/code/KPF-Pipeline/static/tsdb_tables'
        self.csv_filepath = self.keyword_base_path + '/tables_metadata.csv'
        self.tables_metadata_df = pd.read_csv(self.csv_filepath).fillna('')
        tables = {}
        for _, row in self.tables_metadata_df.iterrows():
            table_name = row['table_name']
            tables[table_name] = {
                'csv': row['csv'] if row['csv'] else None,
                'label': row['label'] if row['label'] else None,
                'file_level': row['file_level'] if row['file_level'] else None,
                'extension': row['extension'] if row['extension'] else None,
                'comment': row['comment'] if row['comment'] else None
            }
        self.tables = tables

        # Dynamically build extraction plan
        self.extraction_plan = {
            table_name: {
                'file_level': details['file_level'],
                'extension': details['extension']
            }
            for table_name, details in self.tables.items()
            if details['file_level'] is not None and details['extension'] is not None
        }
        
        # Initialize metadata entries first
        self._init_metadata_entries()

        # Create metadata table or use existing one
        metadata_table = 'tsdb_metadata'
        if not self.check_if_table_exists(tablename=metadata_table):
            self.logger.info("Metadata table does not exist.  Attempting to create.")
            self._create_metadata_table()
        else:
            self.logger.info("Metadata table exists.")
        self._read_metadata_table()
        self._set_boolean_columns()
        self.kw_to_table = {keyword: table_name for keyword, _, table_name in self.metadata_rows}
        self.kw_to_dtype = {keyword: datatype for keyword, datatype, _ in self.metadata_rows}
        self.keywords_by_table = {}
        for keyword, table in self.kw_to_table.items():
            self.keywords_by_table.setdefault(table, []).append(keyword)

        # Create the data tables using metadata
        primary_table = 'tsdb_base'
        if not self.check_if_table_exists(tablename=primary_table):
            self.logger.info("Data tables do not exist.  Attempting to create.")
            self._create_data_tables()
        else:
            self.logger.info("Data tables exist.")


    def require_role(allowed_roles=None):
        """
        Decorator that restricts method execution based on backend and user roles.
    
        Args:
            allowed_roles (list or None): List of roles that have access.
                - If None, only sqlite backend is allowed.
                - If a list, psql backend must match one of the roles in the list.
        """
        def decorator(func):
            @wraps(func)
            def wrapper(self, *args, **kwargs):
                method_name = func.__name__
                
                if self.backend == 'sqlite':
                    # SQLite backend always allowed if allowed_roles is None or explicitly allowed.
                    return func(self, *args, **kwargs)
    
                elif self.backend == 'psql':
                    if not hasattr(self, 'user_role'):
                        self.user_role = self.get_user_role()
    
                    if allowed_roles and self.user_role in allowed_roles:
                        return func(self, *args, **kwargs)
                    else:
                        allowed_str = ', '.join(allowed_roles) if allowed_roles else 'No roles'
                        raise PermissionError(
                            f"Method '{method_name}' not allowed for role '{self.user_role}'. "
                            f"Allowed roles: {allowed_str}"
                        )
    
                else:
                    raise ValueError(f"Unsupported backend: {self.backend}")
    
            return wrapper
        return decorator


    def _open_connection(self):
        """
        Open the database connection
        """

        if self.backend == 'sqlite':
            self.conn = sqlite3.connect(self.db_path)
            self.cursor = self.conn.cursor()
        elif self.backend == 'psql':
            try:
                db_fail = True
                n_attempts = 3
                for i in range(n_attempts):
                    try: 
                        self.conn = psycopg2.connect(
                            host=self.dbserver,
                            database=self.dbname,
                            port=self.dbport,
                            user=self.dbuser,
                            password=self.dbpass
                        )
                        db_fail = False
                        break
                    except:
                        self.logger.info("Could not connect to database, retrying...")
                        db_fail = True
                        time.sleep(10)
                self.cursor = self.conn.cursor()
            except Exception as e:
                self.logger.error(f"Failed to connect to PostgreSQL: {e}")
                raise
                
    
    def _close_connection(self):
        """
        Close the database connection.
        """

        if self.conn:
            if self.backend == 'sqlite':
                self.conn.commit()
            elif self.backend == 'psql':
                self.conn.commit()
            self.cursor.close()
            self.conn.close()
            self.conn = None
            self.cursor = None

    
    def _execute_sql_command(self, command, params=None, fetch=False):
        """
        Add docstring.
        """

        if self.cursor is None:
            raise RuntimeError("Database connection is not open.")
    
        try:
            if params:
                if self.backend == 'sqlite':
                    self.cursor.execute(command, params)
                elif self.backend == 'psql':
                    self.cursor.execute(command.replace('?', '%s'), params)
            else:
                self.cursor.execute(command)
    
            if fetch:
                return self.cursor.fetchall()
        except Exception as e:
            self.logger.error(f"SQL Execution error: {e}\nCommand: {command}\nParams: {params}")
            raise


    def get_user_role(self):
        if self.backend != 'psql':
            raise ValueError("get_user_role only supported with PostgreSQL backend")
    
        schema_perms = self._check_user_schema_permissions()
    
        if schema_perms['SUPERUSER']:
            return 'admin'
        elif schema_perms['CREATE'] and schema_perms['USAGE']:
            return 'operations'
        elif schema_perms['USAGE']:
            return 'readonly'
        else:
            return 'none'


    def _check_user_schema_permissions(self, schema_name='public'):
        """
        Checks schema-level permissions (usage and create) independently of any table.
    
        Args:
            schema_name (str): Schema name to check permissions for.
    
        Returns:
            dict: User permissions indicating access capabilities.
        """
        permissions = {
            'USAGE': False,
            'CREATE': False,
            'SUPERUSER': False
        }
    
        self._open_connection()
        try:
            # Check schema-level privileges directly
            self.cursor.execute("""
                SELECT
                    has_schema_privilege(current_user, %s, 'USAGE'),
                    has_schema_privilege(current_user, %s, 'CREATE'),
                    rolsuper
                FROM pg_roles
                WHERE rolname = current_user;
            """, (schema_name, schema_name))
    
            result = self.cursor.fetchone()
            if result:
                permissions['USAGE'], permissions['CREATE'], permissions['SUPERUSER'] = result
    
        except psycopg2.Error as e:
            self.logger.error(f"Error checking schema permissions: {e}")
    
        finally:
            self._close_connection()
    
        return permissions


    @require_role(['admin', 'operations', 'readonly'])
    def print_summary_all_tables(self):
        """
        Prints a summary of all tables in the database (not just the intended tables), 
        including the number of rows and columns.
        """

        self._open_connection()
        try:
            if self.backend == 'sqlite':
                self._execute_sql_command("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;")
                tables = [row[0] for row in self.cursor.fetchall()]
    
                print(f"{'Table Name':<20} {'Columns':>7} {'Rows':>10}")
                print("-" * 40)
    
                for tbl in tables:
                    self._execute_sql_command(f"PRAGMA table_info({tbl});")
                    columns = len(self.cursor.fetchall())
    
                    self._execute_sql_command(f"SELECT COUNT(*) FROM {tbl};")
                    rows = self.cursor.fetchone()[0]
    
                    print(f"{tbl:<20} {columns:>7} {rows:>10}")
    
            elif self.backend == 'psql':
                if not self.user_role in ['admin', 'operations', 'readonly']:
                    self.logger.error(f"User role = '{self.user_role}' is not sufficient for the method '{inspect.currentframe().f_code.co_name}'.")
                else:
                    self._execute_sql_command("""
                         SELECT tablename FROM pg_catalog.pg_tables
                         WHERE schemaname='public' ORDER BY tablename;
                    """)
                    tables = [row[0] for row in self.cursor.fetchall()]
         
                    print(f"{'Table Name':<20} {'Columns':>7} {'Rows':>10}")
                    print("-" * 40)
         
                    for tbl in tables:
                        self._execute_sql_command(f"""
                            SELECT COUNT(*) FROM information_schema.columns
                            WHERE table_name = '{tbl}';
                        """)
                        columns = self.cursor.fetchone()[0]
         
                        self._execute_sql_command(f"SELECT COUNT(*) FROM {tbl};")
                        rows = self.cursor.fetchone()[0]
         
                        print(f"{tbl:<20} {columns:>7} {rows:>10}")
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def drop_tables(self, tables = 'all'):
        """
        Start over on the database by dropping all data and metadata tables.
        """
        if tables == 'all':
            tables = self.tables
        
        self._open_connection()
        try:
            for tbl in tables:
                self._execute_sql_command(f"DROP TABLE IF EXISTS {tbl}")
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def unlock_db(self):
        """
        Remove the -wal and -shm lock files for SQLite databases.
        For PostgreSQL, prints a message indicating that unlocking is not required.
        """
        if self.backend == 'sqlite':
            wal_file = f"{self.db_path}-wal"
            shm_file = f"{self.db_path}-shm"
    
            if os.path.exists(wal_file):
                os.remove(wal_file)
                self.logger.info(f"File removed: {wal_file}")
            if os.path.exists(shm_file):
                os.remove(shm_file)
                self.logger.info(f"File removed: {shm_file}")
        elif self.backend == 'psql':
            self.logger.info("unlock_db() is not applicable for PostgreSQL databases.")


    @require_role(['admin', 'operations', 'readonly'])
    def check_if_table_exists(self, tablename=None):
        """
        Check if the specified table exists in the database.
        """
        if tablename is None:
            self.logger.info('check_if_table_exists: tablename not specified.')
            return False
    
        self._open_connection()
        try:
            if self.backend == 'sqlite':
                query = "SELECT name FROM sqlite_master WHERE type='table' AND name=?;"
            elif self.backend == 'psql':
                query = "SELECT table_name FROM information_schema.tables WHERE table_schema='public' AND table_name=%s;"
    
            result = self._execute_sql_command(query, params=(tablename,), fetch=True)
            return len(result) > 0
    
        except Exception as e:
            self.logger.error(f"Error checking table existence: {e}")
            return False
        finally:
            self._close_connection()


    def _init_metadata_entries(self):
        """
        Load and combine all keyword metadata entries from CSV files into self.metadata_entries.
        """
        dfs = []
        tables_metadata_df = pd.read_csv(self.csv_filepath).fillna('')
    
        for _, row in tables_metadata_df.iterrows():
            csv_filename = row['csv']
            label = row['label']
            table_name = row['table_name']
            
            if not csv_filename:
                continue  # skip if CSV not specified
    
            csv_path = os.path.join(self.keyword_base_path, csv_filename)
            df = pd.read_csv(csv_path, delimiter='|', dtype=str).fillna('')
            df['source'] = label
            df['table_name'] = table_name
            df['csv_path'] = csv_path
            df.rename(columns={'unit': 'units'}, inplace=True)
    
            dfs.append(df[['keyword', 'datatype', 'units', 'description', 'source', 'table_name', 'csv_path']])
    
        # Combine all dataframes into one
        df_all = pd.concat(dfs, ignore_index=True)
        df_all.drop_duplicates(subset='keyword', inplace=True)
    
        self.metadata_entries = df_all.to_dict(orient='records')

   
    @require_role(['admin', 'operations', 'readonly'])
    def print_db_status(self):
        """
        Prints a formatted summary table of the database status for each table,
        ensuring tables exist first, and handling both SQLite and PostgreSQL.
        """
        tables = [table for table in self.tables if table != 'tsdb_metadata']
    
        self._open_connection()
    
        summary_data = []
    
        try:
            for table in tables:
                # Verify table existence first
                table_exists = False
    
                if self.backend == 'sqlite':
                    self._execute_sql_command(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name=?;",
                        params=(table,)
                    )
                    table_exists = bool(self.cursor.fetchone())
                elif self.backend == 'psql':
                    self._execute_sql_command(
                        """
                        SELECT EXISTS (
                            SELECT 1 FROM information_schema.tables
                            WHERE table_name=%s
                        );
                        """, params=(table,)
                    )
                    table_exists = self.cursor.fetchone()[0]
    
                if not table_exists:
                    self.logger.warning(f"Table '{table}' does not exist; skipping.")
                    continue
    
                self._execute_sql_command(f'SELECT COUNT(*) FROM {table}')
                nrows = self.cursor.fetchone()[0]
    
                if self.backend == 'sqlite':
                    self._execute_sql_command(f'PRAGMA table_info({table})')
                    ncolumns = len(self.cursor.fetchall())
                elif self.backend == 'psql':
                    self._execute_sql_command(
                        """
                        SELECT COUNT(*) FROM information_schema.columns
                        WHERE table_name=%s;
                        """, params=(table,)
                    )
                    ncolumns = self.cursor.fetchone()[0]
    
                summary_data.append((table, ncolumns, nrows))
    
            if not summary_data:
                self.logger.info("No tables exist.")
                return
    
            # Fetch additional stats if 'tsdb_base' exists
            if 'tsdb_base' in [t[0] for t in summary_data]:
                query_time_col = 'L0_header_read_time' if self.backend == 'sqlite' else '"L0_header_read_time"'
                query_datecode_col = 'datecode' if self.backend == 'sqlite' else '"datecode"'
    
                self._execute_sql_command(
                    f'SELECT MAX({query_time_col}) FROM tsdb_base'
                )
                most_recent_read_time = self.cursor.fetchone()[0] or 'N/A'
    
                self._execute_sql_command(
                    f'SELECT MIN({query_datecode_col}), MAX({query_datecode_col}), COUNT(DISTINCT {query_datecode_col}) FROM tsdb_base'
                )
                earliest_datecode, latest_datecode, unique_datecodes_count = self.cursor.fetchone()
    
                earliest_datecode = earliest_datecode or 'N/A'
                latest_datecode = latest_datecode or 'N/A'
                unique_datecodes_count = unique_datecodes_count or 0
            else:
                most_recent_read_time = earliest_datecode = latest_datecode = 'N/A'
                unique_datecodes_count = 0
    
            # Print the summary table
            self.logger.info("Database Table Summary:")
            self.logger.info(f"{'Table':<15} {'Columns':>7} {'Rows':>10}")
            self.logger.info("-" * 35)
            for table, cols, rows in summary_data:
                self.logger.info(f"{table:<15} {cols:>7} {rows:>10}")
    
            # Print additional stats
            self.logger.info(f"Dates: {unique_datecodes_count} days from {earliest_datecode} to {latest_datecode}")
            self.logger.info(f"Last update: {most_recent_read_time}")
    
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def _create_metadata_table(self):
        """
        Create the tsdb_metadata table with table_name mapping from tables_metadata.csv.
        """
        self._open_connection()
        try:
            # Drop the existing metadata table if it exists
            self._execute_sql_command("DROP TABLE IF EXISTS tsdb_metadata")
            
            # Create the new metadata table
            create_sql = """
                CREATE TABLE tsdb_metadata (
                    keyword     TEXT PRIMARY KEY,
                    source      TEXT,
                    datatype    TEXT,
                    units       TEXT,
                    description TEXT,
                    table_name  TEXT
                );
            """
            self._execute_sql_command(create_sql)
    
            # Backend-specific insert command
            insert_sql = """
                INSERT INTO tsdb_metadata 
                (keyword, source, datatype, units, description, table_name)
                VALUES (?, ?, ?, ?, ?, ?);
            """ if self.backend == 'sqlite' else """
                INSERT INTO tsdb_metadata 
                (keyword, source, datatype, units, description, table_name)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (keyword) DO UPDATE SET
                    source=EXCLUDED.source,
                    datatype=EXCLUDED.datatype,
                    units=EXCLUDED.units,
                    description=EXCLUDED.description,
                    table_name=EXCLUDED.table_name;
            """
    
            # Load the tables metadata CSV
            tables_metadata_df = pd.read_csv(self.csv_filepath).fillna('')
    
            # Iterate over each row in tables_metadata.csv
            for _, row in tables_metadata_df.iterrows():
                csv_filename = row['csv']
                source = row['label']
                table_name = row['table_name']
    
                # Skip if no CSV file specified
                if not csv_filename:
                    continue
    
                csv_path = os.path.join(self.keyword_base_path, csv_filename)
    
                # Read keywords CSV with delimiter '|'
                df_keywords = pd.read_csv(csv_path, delimiter='|', dtype=str).fillna('')
    
                # Insert each keyword into metadata table
                for _, keyword_row in df_keywords.iterrows():
                    keyword = keyword_row['keyword']
                    datatype = keyword_row.get('datatype', 'TEXT')
                    units = keyword_row.get('unit', '')
                    description = keyword_row.get('description', '')
    
                    # Execute SQL insert
                    self._execute_sql_command(
                        insert_sql,
                        params=(keyword, source, datatype, units, description, table_name)
                    )
    
        finally:
            self._close_connection()
    
        self.logger.info("Metadata table created correctly.")


    @require_role(['admin', 'operations', 'readonly'])
    def _read_metadata_table(self):
        """
        Read the tsdb_metadata table and store it in the attribute self.metadata_table.
        """
        sql_metadata = "SELECT keyword, datatype, table_name FROM tsdb_metadata;"
        self._open_connection()
        self.metadata_rows = self._execute_sql_command(sql_metadata, fetch=True)
        self._close_connection()
        self.logger.info("Metadata table read.")


    @require_role(['admin', 'operations'])
    def _create_data_tables(self):
        """
        Create TSDB data tables split by category with ObsID as primary key.
        """
    
        # Fetch keyword, datatype, and table_name from metadata
        tables = [table for table in self.tables if table != 'tsdb_metadata']
        sql_metadata = "SELECT keyword, datatype, table_name FROM tsdb_metadata;"
        self._open_connection()
        metadata_rows = self._execute_sql_command(sql_metadata, fetch=True)
    
        columns_by_table = {tbl: [] for tbl in tables}
        for keyword, dtype, table_name in metadata_rows:
            if table_name not in columns_by_table or table_name is None:
                continue
            if keyword.strip().lower() == 'obsid':
                continue  # ObsID is primary key
            sql_type = self._map_data_type_to_sql(dtype)
            columns_by_table[table_name].append((keyword, sql_type))
    
        # Create each table with ObsID as primary key (implicitly indexed)
        for tbl, cols in columns_by_table.items():
            col_defs = ['"ObsID" TEXT PRIMARY KEY']
            col_defs += [f'"{kw}" {sql_type}' for kw, sql_type in cols]
            col_defs_sql = ", ".join(col_defs)
        
            create_table_sql = f"CREATE TABLE IF NOT EXISTS {tbl} ({col_defs_sql});"
            self._execute_sql_command(create_table_sql)
    
        self._close_connection()
    
        self.logger.info("Data tables created.")
 
 
    @require_role(['admin', 'operations'])
    def create_test_table(tablename, schema='public'):
        """
        Create a table to test database functionality.
        """
        
        self._open_connection()
    
        try:
            # Specify schema explicitly
            cursor.execute(f"CREATE TABLE {schema}.test (id integer);")
            conn.commit()
            print(f"CREATE TABLE {schema}.{tablename}")
    
        except psycopg2.Error as e:
            print(f"Error creating table: {e}")
            conn.rollback()
    
        finally:
            self._close_connection()
           

    @require_role(['admin', 'operations', 'readonly'])
    def _set_boolean_columns(self):
        """
        Set the self.bool_columns attribute with the names of all database columns 
        that should be treated as booleans, based on the metadata table.
        """
        self._open_connection()
        try:
            self.bool_rows = self._execute_sql_command(
                "SELECT keyword, datatype, table_name FROM tsdb_metadata WHERE datatype = 'bool';",
                fetch=True
            )
            self.bool_columns = {row[0] for row in self.bool_rows}
            if self.verbose:
                self.logger.debug(f"Boolean columns set: {self.bool_columns}")
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def ingest_one_observation(self, dir_path, L0_filename):
        """
        Ingest a single observation into the database using dynamic 
        keyword-table mapping from metadata.
    
        Args:
            dir_path (str): Path to the directory containing the files.
            L0_filename (str): Filename of the L0 FITS file.
        """
        def safe_float(value):
            try:
                return float(value)
            except (ValueError, TypeError):
                return None  # or np.nan

        base_filename = L0_filename.split('.fits')[0]
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
        extraction_results = {}
        
        for tbl, plan in self.extraction_plan.items():
            file_level = plan['file_level']
            ext = plan['extension']
            file_path = (
                f"{dir_path.replace('L0', file_level)}/{base_filename}_{file_level}.fits"
                if file_level != 'L0'
                else f"{dir_path}/{base_filename}.fits"
            )
     
            keywords = self.keywords_by_table.get(tbl, [])
    
            if tbl == 'tsdb_l0t':
                extracted = self._extract_telemetry(file_path, {kw: self.kw_to_dtype[kw] for kw in keywords})
            elif tbl.startswith('tsdb_l2_') and tbl not in ['tsdb_l2', 'tsdb_l2_rv', 'tsdb_l2_ccf']:
                extracted = self._extract_rvs(file_path)
            else:
                kw_types = {kw: self.kw_to_dtype[kw] for kw in keywords}
                extracted = self._extract_kwd(file_path, kw_types, ext)
    
            extraction_results.update(extracted)
    
        extraction_results.update({
            'ObsID': base_filename,
            'datecode': get_datecode(base_filename),
            'Source': self.get_source(extraction_results),
            'L0_filename': f"{base_filename}.fits",
            'D2_filename': f"{base_filename}_2D.fits",
            'L1_filename': f"{base_filename}_L1.fits",
            'L2_filename': f"{base_filename}_L2.fits",
            'L0_header_read_time': now_str,
            'D2_header_read_time': now_str,
            'L1_header_read_time': now_str,
            'L2_header_read_time': now_str
        })
    
        for kw, dtype in self.kw_to_dtype.items():
            if dtype == 'float' and kw in extraction_results:
                extraction_results[kw] = safe_float(extraction_results[kw])
    
        self._open_connection()
        try:
            table_data = {}
            for kw, value in extraction_results.items():
                table_name = self.kw_to_table.get(kw)
                if table_name:
                    table_data.setdefault(table_name, {})[kw] = value
    
            for tbl, data in table_data.items():
                for kw in data:
                    if kw in self.bool_columns:
                        val = data[kw]
                        data[kw] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']
                data['ObsID'] = base_filename
    
            for tbl, data in table_data.items():
                columns = ', '.join(f'"{col}"' for col in data)
                placeholders = ', '.join(['?'] * len(data)) if self.backend == 'sqlite' else ', '.join(['%s'] * len(data))
    
                if self.backend == 'sqlite':
                    insert_query = f'INSERT OR REPLACE INTO {tbl} ({columns}) VALUES ({placeholders})'
                else:
                    updates = ', '.join(f'"{col}"=EXCLUDED."{col}"' for col in data if col != 'ObsID')
                    insert_query = (
                        f'INSERT INTO {tbl} ({columns}) VALUES ({placeholders}) '
                        f'ON CONFLICT ("ObsID") DO UPDATE SET {updates}'
                    )
    
                self._execute_sql_command(insert_query, params=tuple(data.values()))
        finally:
            self._close_connection()
    
        self.logger.info(f"Ingested observation: {base_filename}")


    @require_role(['admin', 'operations'])
    def ingest_dates_to_db(self, start_date_str, end_date_str, batch_size=10000, reverse=False, force_ingest=False, quiet=False):
        """
        Ingest KPF data for the date range start_date to end_date, inclusive.
        batch_size refers to the number of observations per DB insertion.
        If force_ingest=False, files are not reingested unless they have more 
        recent modification dates than in DB.
        """

        # Convert input dates to strings if necessary
        if isinstance(start_date_str, datetime):
            start_date_str = start_date_str.strftime("%Y%m%d")
        if isinstance(end_date_str, datetime):
            end_date_str = end_date_str.strftime("%Y%m%d")
        
        if not quiet:
            self.logger.info("Adding to database between " + start_date_str + " and " + end_date_str)
        dir_paths = glob.glob(f"{self.base_dir}/????????")
        sorted_dir_paths = sorted(dir_paths, key=lambda x: int(os.path.basename(x)), reverse=start_date_str > end_date_str)
        filtered_dir_paths = [
            dir_path for dir_path in sorted_dir_paths
            if start_date_str <= os.path.basename(dir_path) <= end_date_str
        ]
        if len(filtered_dir_paths) > 0:
            # Reverse dates if the reverse flag is set
            if reverse:
                filtered_dir_paths.reverse()
            
            # Iterate over date directories
            t1 = self.tqdm(filtered_dir_paths, desc=(filtered_dir_paths[0]).split('/')[-1], disable=quiet)
            for dir_path in t1:
                t1.set_description(dir_path.split('/')[-1])
                t1.refresh() 
                t2 = self.tqdm(os.listdir(dir_path), desc=f'Files', leave=False, disable=quiet)
                batch = []
                for L0_filename in t2:
                    if L0_filename.endswith(".fits"):
                        file_path = os.path.join(dir_path, L0_filename)
                        batch.append(file_path)
                        if len(batch) >= batch_size:
                            self._ingest_batch_observations(batch, force_ingest=force_ingest)
                            batch = []
                if batch:
                    self._ingest_batch_observations(batch, force_ingest=force_ingest)

        if not quiet:
            self.logger.info(f"Files for {len(filtered_dir_paths)} days ingested/checked")


    @require_role(['admin', 'operations'])
    def _ingest_batch_observations(self, batch, force_ingest=False):
        """
        Ingest a batch of observations into the multi-table database, dynamically handling L1 tables and metadata.
    
        Args:
            batch (list): List of file paths for the batch.
            force_ingest (bool): Force ingestion regardless of file modification checks.
        """

        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
        # Filter updated files
        updated_batch = batch if force_ingest else [
            file_path for file_path in batch if self._is_any_file_updated(file_path)
        ]
        if not updated_batch:
            if self.verbose:
                self.logger.info("No new or updated files found in batch.")
            return
    
        extraction_args = {
            'now_str': now_str,
            'extraction_plan': self.extraction_plan,
            'bool_columns': self.bool_columns,
            'kw_to_table': self.kw_to_table,
            'keywords_by_table': self.keywords_by_table,
            'kw_to_dtype': self.kw_to_dtype,
            '_extract_kwd_func': self._extract_kwd,
            '_extract_telemetry_func': self._extract_telemetry,
            '_extract_rvs_func': self._extract_rvs,
            'get_source_func': self.get_source,
            'get_datecode_func': get_datecode
        }
    
        partial_process_file = partial(
            process_file, 
            safe_float=safe_float, 
            **extraction_args
        )
    
        max_workers = min(len(updated_batch), 20, os.cpu_count())
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(partial_process_file, updated_batch))
    
        valid_results = [res for res in results if res]
    
        if not valid_results:
            self.logger.warning("No valid results extracted from batch files.")
            return
    
        self._open_connection()
        try:
            table_data = {}
            for obs_data in valid_results:
                obsid = obs_data['ObsID']
                for kw, value in obs_data.items():
                    table = self.kw_to_table.get(kw)
                    if table:
                        table_data.setdefault(table, []).append((obsid, kw, value))
    
            for tbl, rows in table_data.items():
                structured_data = {}
                for obsid, kw, val in rows:
                    structured_data.setdefault(obsid, {'ObsID': obsid})[kw] = val
    
                # Boolean handling
                for data in structured_data.values():
                    for kw in data:
                        if kw in self.bool_columns:
                            val = data[kw]
                            data[kw] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']
    
                columns = list(next(iter(structured_data.values())).keys())
                col_str = ', '.join(f'"{col}"' for col in columns)
    
                if self.backend == 'sqlite':
                    placeholder_char = '?'
                    placeholders = ', '.join([placeholder_char] * len(columns))
                    insert_query = f'INSERT OR REPLACE INTO {tbl} ({col_str}) VALUES ({placeholders})'
                else:
                    placeholder_char = '%s'
                    placeholders = ', '.join([placeholder_char] * len(columns))
                    updates = ', '.join(f'"{col}" = EXCLUDED."{col}"' for col in columns if col != 'ObsID')
                    conflict_clause = (
                        f'ON CONFLICT ("ObsID") DO UPDATE SET {updates}' if updates else 'ON CONFLICT ("ObsID") DO NOTHING'
                    )
                    insert_query = f'INSERT INTO {tbl} ({col_str}) VALUES ({placeholders}) {conflict_clause}'
    
                data_tuples = [tuple(data[col] for col in columns) for data in structured_data.values()]
    
                try:
                    self.cursor.executemany(insert_query, data_tuples)
                except Exception as e:
                    self.logger.error(f"Bulk insert error in table {tbl}: {e}")
                    for data_tuple in data_tuples:
                        try:
                            self._execute_sql_command(insert_query, params=data_tuple)
                        except Exception as single_e:
                            problematic_obsid = data_tuple[columns.index('ObsID')]
                            self.logger.error(f"Insert failed for ObsID {problematic_obsid} in table {tbl}: {single_e}")
    
            self.conn.commit()
    
        finally:
            self._close_connection()


    def _map_data_type_to_sql(self, dtype):
        """
        Function to map the data types specified in get_keyword_types to 
        sqlite and psql datatypes.
        """
        if self.backend == 'sqlite':
            return {
                'int': 'INTEGER',
                'float': 'REAL',
                'bool': 'BOOLEAN',
                'datetime': 'TEXT',  # SQLite does not have a native datetime type
                'string': 'TEXT'
            }.get(dtype, 'TEXT')
        elif self.backend == 'psql':
            return {
                'int': 'INTEGER',
                'float': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime': 'TIMESTAMP',
                'string': 'TEXT'
            }.get(dtype, 'TEXT')
        else:
            raise ValueError(f"Unsupported backend: {self.backend}")


    def get_source(self, L0_dict):
        """
        Returns the name of the source in a spectrum.  For stellar observations, this 
        it returns 'Star'.  For calibration spectra, this is the lamp name 
        (ThAr, UNe, LFC, etalon) or bias/dark.  
        Flats using KPF's regular fibers are distinguished from wide flats.                           

        Returns:
            the source/image name
            possible values: 'Bias', 'Dark', 'Flat', 'Wide Flat', 
                             'LFC', 'Etalon', 'ThAr', 'UNe',
                             'Sun', 'Star'
        """
        try: 
            if (('ELAPSED' in L0_dict) and 
                ((L0_dict['IMTYPE'] == 'Bias') or (L0_dict['ELAPSED'] == 0))):
                    return 'Bias'
            elif L0_dict['IMTYPE'] == 'Dark':
                return 'Dark' 
            elif L0_dict['FFFB'].strip().lower() == 'yes':
                    return 'Wide Flat' # Flatfield Fiber (wide flats)
            elif L0_dict['IMTYPE'].strip().lower() == 'flatlamp':
                 if 'brdband' in L0_dict['OCTAGON'].strip().lower():
                    return 'Flat' # Flat through regular fibers
            elif L0_dict['IMTYPE'].strip().lower() == 'arclamp':
                if 'lfc' in L0_dict['OCTAGON'].strip().lower():
                    return 'LFC'
                if 'etalon' in L0_dict['OCTAGON'].strip().lower():
                    return 'Etalon'
                if 'th_' in L0_dict['OCTAGON'].strip().lower():
                    return 'ThAr'
                if 'u_' in L0_dict['OCTAGON'].strip().lower():
                    return 'UNe'
            elif ((L0_dict['TARGNAME'].strip().lower() == 'sun') or 
                  (L0_dict['TARGNAME'].strip().lower() == 'socal')):
                return 'Sun' # SoCal
            if ('OBJECT' in L0_dict) and ('FIUMODE' in L0_dict):
                if (L0_dict['FIUMODE'] == 'Observing'):
                    return 'Star'
        except:
            return 'Unknown'
        
    
    def _extract_kwd(self, file_path, keyword_types, extension='PRIMARY'):
        """
        Extract keywords from keyword_types.keys from an extension in a L0/2D/L1/L2 file.
        Additionally, if DRPTAG is valid, populate DRPTAG2D, DRPTAGL1, and DRPTAGL2 with its value.
        """
        # Initialize the result dictionary with None for all keywords
        header_data = {key: None for key in keyword_types.keys()}
    
        # Check if the file exists before proceeding
        if not os.path.isfile(file_path):
            return header_data
    
        try:
            # Open the FITS file and read the specified header
            with fits.open(file_path, memmap=True) as hdul:
                header = hdul[extension].header
    
                # Populate header_data from header
                header_data = {key: header.get(key, None) for key in keyword_types.keys()}
    
                # If DRPTAG is valid, propagate its value to appropriate data level
                drptag_value = header.get('DRPTAG', None)
                if drptag_value is not None:
                    for target_key in ['DRPTAG2D', 'DRPTAGL1', 'DRPTAGL2']:
                        if target_key in header_data:
                            header_data[target_key] = drptag_value
    
                # If DRPHASH is valid, propagate its value to appropriate data level
                drphash_value = header.get('DRPHASH', None)
                if drphash_value is not None:
                    for target_key in ['DRPHSH2D', 'DRPHSHL1', 'DRPHSHL2']:
                        if target_key in header_data:
                            header_data[target_key] = drphash_value
    
        except Exception as e:
            self.logger.error(f"Bad file: {file_path}. Error: {e}")
    
        return header_data


    def _extract_telemetry(self, file_path, keyword_types):
        """
        Extract telemetry from the 'TELEMETRY' extension in a KPF L0 file.
        """
        try:
            # Use astropy's Table to load only necessary data
            telemetry_table = Table.read(file_path, format='fits', hdu='TELEMETRY')
            keywords = telemetry_table['keyword']
            averages = telemetry_table['average']
        except Exception as e:
            self.logger.info(f"Bad TELEMETRY extension in: {file_path}. Error: {e}")
            return {key: None for key in keyword_types}
    
        try:
            # Decode and sanitize 'keyword' column
            keywords = [k.decode('utf-8') if isinstance(k, bytes) else k for k in keywords]
    
            # Replace invalid values efficiently using NumPy
            averages = np.array(averages, dtype=object)  # Convert to object to allow mixed types
            mask_invalid = np.isin(averages, ['-nan', 'nan', -999]) | np.isnan(pd.to_numeric(averages, errors='coerce'))
            averages[mask_invalid] = np.nan
            averages = averages.astype(float)  # Convert valid data to float
    
            # Create the telemetry dictionary
            telemetry_data = dict(zip(keywords, averages))
    
            # Build the output dictionary for the requested keywords
            telemetry_dict = {key: float(telemetry_data.get(key, np.nan)) for key in keyword_types}
        except Exception as e:
            self.logger.info(f"Error processing TELEMETRY data in: {file_path}. Error: {e}")
            telemetry_dict = {key: None for key in keyword_types}
    
        return telemetry_dict


    def _extract_rvs(self, file_path):
        """
        Extract RVs from the 'RV' extension in a KPF L2 file.
        """
        mapping = {
            'orderlet1':   'RV1{}',
            'orderlet2':   'RV2{}',
            'orderlet3':   'RV3{}',
            'RV':          'RVS{}',
            'RV error':    'ERVS{}',
            'CAL RV':      'RVC{}',
            'CAL error':   'ERVC{}',
            'SKY RV':      'RVY{}',
            'SKY error':   'ERVY{}',
            'CCFBJD':      'CCFBJD{}',
            'Bary_RVC':    'BCRV{}',
            'CCF Weights': 'CCFW{}',
        }
    
        cols = ['orderlet1', 'orderlet2', 'orderlet3', 'RV', 'RV error', 
                'CAL RV', 'CAL error', 'SKY RV', 'SKY error', 'CCFBJD', 
                'Bary_RVC', 'CCF Weights']
    
        expected_count = 67 * len(cols)
    
        def make_dummy_dict():
            keys = []
            for i in range(0, 67):
                NN = f"{i:02d}"  # two-digit row number, from 00 to 66
                for pattern in mapping.values():
                    keys.append(pattern.format(NN))
            return {key: None for key in keys}
        
        try:
            df_rv = Table.read(file_path, format='fits', hdu='RV').to_pandas()
            df_rv = df_rv[cols]
        except Exception as e:
            # If we can't read RVs, return a dict with None values for all expected keys
            rv_dict = make_dummy_dict()
            return rv_dict
    
        df_filtered = df_rv[list(mapping.keys())]
        stacked = df_filtered.stack()
        keyed = stacked.reset_index()
        keyed.columns = ['row_idx', 'col', 'val']
        keyed['NN'] = keyed['row_idx'].apply(lambda x: f"{x:02d}")  # zero-based indexing
        keyed['key'] = keyed['col'].map(mapping)
        keyed['key'] = keyed['key'].str[:-2] + keyed['NN']
        rv_dict = dict(zip(keyed['key'], keyed['val']))
    
        # Check the count - if data wasn't computed for Green or Red, the database will give an error on insertion
        if len(rv_dict) != expected_count:
            # If count doesn't match, return the dummy dictionary of None values
            rv_dict = make_dummy_dict()
            
        return rv_dict        


    def clean_df(self, df):
        """
        Remove known outliers from a dataframe.
        """
        # CCD Read Noise
        cols = ['RNGREEN1', 'RNGREEN2', 'RNGREEN3', 'RNGREEN4', 'RNRED1', 'RNRED2', 'RNRED3', 'RNRED4']
        for col in cols:
            if col in df.columns:
                df = df.loc[df[col] < 500]
        
        # Hallway temperature
        if 'kpfmet.TEMP' in df.columns:
            df = df.loc[df['kpfmet.TEMP'] > 15]
        
        # Fiber temperatures
        kwrds = ['kpfmet.SIMCAL_FIBER_STG', 'kpfmet.SIMCAL_FIBER_STG']
        for key in kwrds:
            if key in df.columns:
                df = df.loc[df[key] > 0]
                       
        # Dark Current
        kwrds = ['FLXCOLLG', 'FLXECHG', 'FLXREG1G', 'FLXREG2G', 'FLXREG3G', 'FLXREG4G', 
                 'FLXREG5G', 'FLXREG6G', 'FLXCOLLR', 'FLXECHR', 'FLXREG1R', 'FLXREG2R', 
                 'FLXREG3R', 'FLXREG4R', 'FLXREG5R', 'FLXREG6R']

        # Spectrometer and other temperatures from kpfmet
        kwrds = ['kpfmet.BENCH_BOTTOM_BETWEEN_CAMERAS', 'kpfmet.BENCH_BOTTOM_COLLIMATOR', 'kpfmet.BENCH_BOTTOM_DCUT', 'kpfmet.BENCH_BOTTOM_ECHELLE', 'kpfmet.BENCH_TOP_BETWEEN_CAMERAS', 'kpfmet.BENCH_TOP_COLL', 'kpfmet.BENCH_TOP_DCUT', 'kpfmet.BENCH_TOP_ECHELLE_CAM', 'kpfmet.CALEM_SCMBLR_CHMBR_END', 'kpfmet.CALEM_SCMBLR_FIBER_END', 'kpfmet.CAL_BENCH', 'kpfmet.CAL_BENCH_BB_SRC', 'kpfmet.CAL_BENCH_BOT', 'kpfmet.CAL_BENCH_ENCL_AIR', 'kpfmet.CAL_BENCH_OCT_MOT', 'kpfmet.CAL_BENCH_TRANS_STG_MOT', 'kpfmet.CAL_RACK_TOP|float', 'kpfmet.CHAMBER_EXT_BOTTOM', 'kpfmet.CHAMBER_EXT_TOP', 'kpfmet.CRYOSTAT_G1', 'kpfmet.CRYOSTAT_G2', 'kpfmet.CRYOSTAT_G3', 'kpfmet.CRYOSTAT_R1', 'kpfmet.CRYOSTAT_R2', 'kpfmet.CRYOSTAT_R3', 'kpfmet.ECHELLE_BOTTOM', 'kpfmet.ECHELLE_TOP', 'kpfmet.FF_SRC', 'kpfmet.GREEN_CAMERA_BOTTOM', 'kpfmet.GREEN_CAMERA_COLLIMATOR', 'kpfmet.GREEN_CAMERA_ECHELLE', 'kpfmet.GREEN_CAMERA_TOP', 'kpfmet.GREEN_GRISM_TOP', 'kpfmet.GREEN_LN2_FLANGE', 'kpfmet.PRIMARY_COLLIMATOR_TOP', 'kpfmet.RED_CAMERA_BOTTOM', 'kpfmet.RED_CAMERA_COLLIMATOR', 'kpfmet.RED_CAMERA_ECHELLE', 'kpfmet.RED_CAMERA_TOP', 'kpfmet.RED_GRISM_TOP', 'kpfmet.RED_LN2_FLANGE', 'kpfmet.REFORMATTER', 'kpfmet.SCIENCE_CAL_FIBER_STG', 'kpfmet.SCISKY_SCMBLR_CHMBR_EN', 'kpfmet.SCISKY_SCMBLR_FIBER_EN', 'kpfmet.SIMCAL_FIBER_STG', 'kpfmet.SKYCAL_FIBER_STG', 'kpfmet.TEMP', 'kpfmet.TH_DAILY', 'kpfmet.TH_GOLD', 'kpfmet.U_DAILY', 'kpfmet.U_GOLD',]
        for key in kwrds:
            if key in df.columns:
                pass
                # need to check on why these values occur
                #df = df.loc[df[key] > -200]
                #df = df.loc[df[key] <  400]

        return df


    @require_role(['admin', 'operations', 'readonly'])
    def _is_any_file_updated(self, L0_file_path):
        """
        Determines if any file from the L0/2D/L1/L2 set has been updated since the last 
        noted modification in the database. Returns True if any file has been modified.
        """
        L0_filename = os.path.basename(L0_file_path)
    
        timestamp_columns = ["L0_header_read_time", "D2_header_read_time", 
                             "L1_header_read_time", "L2_header_read_time"]
    
        if self.backend == 'psql':
            # Quote columns explicitly for PostgreSQL
            col_str = ', '.join(f'"{col}"' for col in timestamp_columns)
            filename_col = '"L0_filename"'
            placeholder = '%s'
        else:
            # SQLite does not need explicit quoting here
            col_str = ', '.join(timestamp_columns)
            filename_col = 'L0_filename'
            placeholder = '?'
    
        query = f'SELECT {col_str} FROM tsdb_base WHERE {filename_col} = {placeholder}'
    
        self._open_connection()
        try:
            result = self._execute_sql_command(query, params=(L0_filename,), fetch=True)
        finally:
            self._close_connection()
    
        if not result:
            return True
    
        result = result[0]
    
        file_paths = {
            'L0': L0_file_path,
            'D2': L0_file_path.replace('L0', '2D').replace('.fits', '_2D.fits'),
            'L1': L0_file_path.replace('L0', 'L1').replace('.fits', '_L1.fits'),
            'L2': L0_file_path.replace('L0', 'L2').replace('.fits', '_L2.fits'),
        }
    
        for idx, (key, path) in enumerate(file_paths.items()):
            try:
                mod_time = datetime.fromtimestamp(os.path.getmtime(path)).strftime("%Y-%m-%d %H:%M:%S")
            except FileNotFoundError:
                mod_time = '1000-01-01 00:00:00'
    
            if mod_time > (result[idx] or '1000-01-01 00:00:00'):
                return True
    
        return False


    @require_role(['admin', 'operations'])
    def add_ObsID_list_to_db(self, ObsID_filename, reverse=False):
        """
        Read a CSV file with ObsID values in the first column and ingest those files
        into the database.  If reverse=True, then they will be ingested in reverse
        chronological order.
        """
        if os.path.isfile(ObsID_filename):
            try:
                df = pd.read_csv(ObsID_filename)
            except Exception as e:
                self.logger.info(f'Problem reading {ObsID_filename}: ' + e)
        else:
            self.logger.info('File missing: ObsID_filename')
        
        ObsID_pattern = r'KP\.20\d{6}\.\d{5}\.\d{2}'
        first_column = df.iloc[:, 0]
        filtered_column = first_column[first_column.str.match(ObsID_pattern)]
        df = filtered_column.to_frame()
        column_name = df.columns[0]
        df.rename(columns={column_name: 'ObsID'}, inplace=True)
        if reverse:
            df = df.sort_values(by='ObsID', ascending=False)
        else:
            df = df.sort_values(by='ObsID', ascending=True)

        self.logger.info(f'{ObsID_filename} read with {str(len(df))} properly formatted ObsIDs.')

        t = self.tqdm(df.iloc[:, 0].tolist(), desc=f'ObsIDs', leave=True)
        for ObsID in t:
            dir_path = self.base_dir + '/' + get_datecode(ObsID) + '/'
            filename = ObsID + '.fits'
            file_path = os.path.join(dir_path, filename)
            base_filename = filename.split('.fits')[0]
            t.set_description(base_filename)
            t.refresh() 
            try:
                if os.path.exists(ObsID_filename):
                    self.ingest_one_observation(dir_path, filename) 
            except Exception as e:
                self.logger.error(e)


    @require_role(['admin', 'operations'])
    def add_ObsIDs_to_db(self, ObsID_list):
        """
        Ingest files into the database from a list of strings 'ObsID_list'.  
        """
        t = self.tqdm(ObsID_list, desc=f'ObsIDs', leave=True)
        for ObsID in t:
            L0_filename = ObsID + '.fits'
            dir_path = self.base_dir + '/' + get_datecode(ObsID) + '/'
            file_path = os.path.join(dir_path, L0_filename)
            base_filename = L0_filename.split('.fits')[0]
            t.set_description(base_filename)
            t.refresh() 
            try:
                self.ingest_one_observation(dir_path, L0_filename) 
            except Exception as e:
                self.logger.error(e)


    @require_role(['admin', 'operations', 'readonly'])
    def print_metadata_table(self):
        """
        Read the tsdb_metadata table, group by 'source', and print out rows
        in fixed-width columns in the custom order below, without printing
        the 'source' column. If units=NaN, it prints a blank.
        """
        # Define your custom order of sources
        custom_order = [
            "Base Keywords",
            "L0 PRIMARY Header",
            "2D PRIMARY Header",
            "L1 PRIMARY Header",
            "L2 PRIMARY Header",
            "L0 TELEMETRY Extension",
            "L2 RV Header",
            "L2 RV Extension",
            "L2 CCF Header"
        ]
    
        col_width_keyword = 35
        col_width_datatype = 9
        col_width_units = 9
        col_width_desc = 90
    
        self._open_connection()
        try:
            for src in custom_order:
                query = """
                    SELECT keyword, datatype, units, description
                    FROM tsdb_metadata
                    WHERE source = ?
                    ORDER BY keyword;
                """
                self._execute_sql_command(query, params=(src,))
                rows = self.cursor.fetchall()
    
                if not rows:
                    continue
    
                print(f"{src}:")
                print("-" * 150)
                print(
                    f"{'Keyword':<{col_width_keyword}} "
                    f"{'Datatype':<{col_width_datatype}} "
                    f"{'Units':<{col_width_units}} "
                    f"{'Description':<{col_width_desc}}"
                )
                print("-" * 150)
    
                for keyword, datatype, units, description in rows:
                    keyword_str = keyword or ""
                    datatype_str = datatype or ""
                    units_str = "" if units in (None, "NaN", "nan", float("nan")) or pd.isna(units) else units
                    desc_str = description or ""
    
                    print(
                        f"{keyword_str:<{col_width_keyword}} "
                        f"{datatype_str:<{col_width_datatype}} "
                        f"{units_str:<{col_width_units}} "
                        f"{desc_str:<{col_width_desc}}"
                    )
                print()
        finally:
            self._close_connection()


    @require_role(['admin', 'operations', 'readonly'])
    def metadata_table_to_df(self):
        """
        Return a dataframe of the metadata table with columns:
            keyword | datatype | units | description | source
        """
        self._open_connection()
        try:
            query = "SELECT keyword, datatype, units, description, source FROM tsdb_metadata"
            self._execute_sql_command(query)
            rows = self.cursor.fetchall()
    
            df = pd.DataFrame(rows, columns=['keyword', 'datatype', 'units', 'description', 'source'])
        finally:
            self._close_connection()
    
        return df


    @require_role(['admin', 'operations', 'readonly'])
    def print_table_contents(self, table_name):
        """
        Print the contents of the specified table.  This is useful for debugging.
    
        Args:
            table_name (str): Name of the table to print.
        """
        self._open_connection()
        try:
            # Fetch all data from the table
            self._execute_sql_command(f"SELECT * FROM {table_name}")
            rows = self.cursor.fetchall()
    
            # Fetch column names
            column_names = [description[0] for description in self.cursor.description]
    
            # Print column names
            print(f"Contents of table '{table_name}':")
            print("-" * 100)
            print('\t'.join(column_names))
            print("-" * 100)
    
            # Print rows
            for row in rows:
                print('\t'.join([str(item) if item is not None else '' for item in row]))
                
            print("-" * 100)
            print(f"Total rows: {len(rows)}")
    
        finally:
            self._close_connection()


    def is_notebook(self):
        """
        Determine if the code is being executed in a Jupyter Notebook.  
        This is useful for tqdm.
        """
        try:
            from IPython import get_ipython
            if 'IPKernelApp' not in get_ipython().config:  # Notebook not running
                return False
        except (ImportError, AttributeError):
            return False  # IPython not installed
        return True


    @require_role(['admin', 'operations', 'readonly'])
    def get_first_last_dates(self):
        """
        Returns a tuple of datetime objects containing the first and last dates 
        in the database. DATE-MID is used for the date.
        """
        self._open_connection()
        try:
            query = """
                SELECT MIN("DATE-MID") AS min_date, MAX("DATE-MID") AS max_date
                FROM tsdb_l0
            """
            self._execute_sql_command(query)
            min_date_str, max_date_str = self.cursor.fetchone()
    
            # Convert strings to datetime objects, handling None values gracefully
            date_format = '%Y-%m-%dT%H:%M:%S.%f'
            first_date = datetime.strptime(min_date_str, date_format) if min_date_str else None
            last_date = datetime.strptime(max_date_str, date_format) if max_date_str else None
        finally:
            self._close_connection()
    
        return first_date, last_date


    @require_role(['admin', 'operations', 'readonly'])
    def display_dataframe_from_db(self, columns, 
                                  only_object=None, object_like=None, only_source=None, 
                                  on_sky=None, not_junk=None,
                                  start_date=None, end_date=None, 
                                  verbose=False):
        """
        Description:
            Print a pandas DataFrame containing specified columns from a 
            joined set of database tables, applying optional filters based on 
            object names, source types, date ranges, sky condition, and quality 
            checks.
    
        Args:
            columns (str or list of str, optional): Column name(s) to retrieve. 
                Defaults to None (fetches all columns).
            start_date (str or datetime, optional): Starting date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            end_date (str or datetime, optional): Ending date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            only_object (str or list of str, optional): Exact object name(s) to 
                filter observations. Defaults to None.
                E.g., only_object = ['autocal-dark', 'autocal-bias']
            only_source (str or list of str, optional): Source type(s) to filter 
                observations. Defaults to None.
                E.g., only_source = ['Dark', 'Bias']
            object_like (str or list of str, optional): Partial object name(s) 
                for filtering observations using SQL LIKE conditions. Defaults 
                to None.
                E.g., object_like = ['autocal-etalon', 'autocal-bias']
            on_sky (bool, optional): Filter by on-sky (True) or calibration 
                (False) observations. Defaults to None.
            not_junk (bool, optional): Filter by observations marked as not junk 
                (True) or junk (False). Defaults to None.
            verbose (bool, optional): Enables detailed logging of SQL queries 
                and parameters. Defaults to False.
    
        Returns:
            None. Prints the resulting dataframe.
        """
        df = self.dataframe_from_db(
            columns=columns,
            only_object=only_object,
            object_like=object_like,
            only_source=only_source,
            on_sky=on_sky, 
            not_junk=not_junk,
            start_date=start_date,
            end_date=end_date,
            verbose=verbose
        )
        print(df)


    @require_role(['admin', 'operations', 'readonly'])
    def dataframe_from_db(self, columns=None, 
                          start_date=None, end_date=None, 
                          only_object=None, only_source=None, object_like=None, 
                          on_sky=None, not_junk=None, 
                          extra_conditions=None,
                          extra_conditions_logic='AND',
                          verbose=False):
        """
        Description:
            Return a Pandas DataFrame containing specified columns from a 
            joined set of database tables, applying optional filters based on 
            object names, source types, date ranges, sky condition, and quality 
            checks.
    
        Args:
            columns (str or list of str, optional): Column name(s) to retrieve. 
                Defaults to None (fetches all columns).
            start_date (str or datetime, optional): Starting date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            end_date (str or datetime, optional): Ending date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            only_object (str or list of str, optional): Exact object name(s) to 
                filter observations. Defaults to None.
                E.g., only_object = ['autocal-dark', 'autocal-bias']
            only_source (str or list of str, optional): Source type(s) to filter 
                observations. Defaults to None.
                E.g., only_source = ['Dark', 'Bias']
            object_like (str or list of str, optional): Partial object name(s) 
                for filtering observations using SQL LIKE conditions. Defaults 
                to None.
                E.g., object_like = ['autocal-etalon', 'autocal-bias']
            on_sky (bool, optional): Filter by on-sky (True) or calibration 
                (False) observations. Defaults to None.
            not_junk (bool, optional): Filter by observations marked as not junk 
                (True) or junk (False). Defaults to None.
            verbose (bool, optional): Enables detailed logging of SQL queries 
                and parameters. Defaults to False.
    
        Returns:
            The resulting dataframe.

        """
    
        if isinstance(only_object, str):
            only_object = [only_object]
        if isinstance(only_source, str):
            only_source = [only_source]
        if isinstance(object_like, str):
            object_like = [object_like]
    
        quote = '"' if self.backend == 'psql' else '"'  # SQLite uses " for quoting as well
        placeholder = '%s' if self.backend == 'psql' else '?'
    
        self._open_connection()
    
        try:
            # Get column metadata
            if columns in (None, '*'):
                metadata_query = 'SELECT keyword, table_name FROM tsdb_metadata;'
                self._execute_sql_command(metadata_query)
                metadata = pd.DataFrame(self.cursor.fetchall(), columns=['keyword', 'table_name'])
                columns_requested = metadata['keyword'].tolist()
            else:
                columns_requested = [columns] if isinstance(columns, str) else columns
                placeholders = ','.join([placeholder] * len(columns_requested))
                metadata_query = f'SELECT keyword, table_name FROM tsdb_metadata WHERE keyword IN ({placeholders});'
                self._execute_sql_command(metadata_query, params=columns_requested)
                metadata = pd.DataFrame(self.cursor.fetchall(), columns=['keyword', 'table_name'])
    
            kw_table_map = dict(zip(metadata['keyword'], metadata['table_name']))
            tables_needed = set(metadata['table_name'].dropna())
            tables_needed.update(['tsdb_base', 'tsdb_l0', 'tsdb_2d'])
    
            # Prepare select columns with proper quoting
            table_columns = {table: {'ObsID'} for table in tables_needed}
            for kw, tbl in kw_table_map.items():
                table_columns[tbl].add(kw)
    
            guaranteed_columns = {
                'tsdb_base': ['Source', 'datecode'],
                'tsdb_l0': ['OBJECT', 'FIUMODE'],
                'tsdb_2d': ['NOTJUNK']
            }
            for tbl, cols in guaranteed_columns.items():
                table_columns[tbl].update(cols)
    
            select_clauses = []
            selected_cols_set = set()
            for tbl in tables_needed:
                for col in table_columns[tbl]:
                    col_quoted = f'{quote}{col}{quote}'
                    alias_quoted = f'{quote}{col}{quote}'
                    if col == 'ObsID' and col in selected_cols_set:
                        continue
                    selected_cols_set.add(col)
                    select_clauses.append(f'{tbl}.{col_quoted} AS {alias_quoted}')
    
            select_sql = ', '.join(select_clauses)
    
            from_clause = 'tsdb_base'
            join_clauses = [
                f'LEFT JOIN {tbl} ON tsdb_base.{quote}ObsID{quote} = {tbl}.{quote}ObsID{quote}'
                for tbl in tables_needed if tbl != 'tsdb_base'
            ]
    
            conditions, params = [], []
    
            if only_object:
                placeholders = ','.join([placeholder] * len(only_object))
                conditions.append(f'tsdb_l0.{quote}OBJECT{quote} IN ({placeholders})')
                params.extend(only_object)
    
            if object_like:
                like_conditions = [f'tsdb_l0.{quote}OBJECT{quote} LIKE {placeholder}' for _ in object_like]
                conditions.append('(' + ' OR '.join(like_conditions) + ')')
                params.extend([f'%{ol}%' for ol in object_like])
    
            if only_source:
                placeholders = ','.join([placeholder] * len(only_source))
                conditions.append(f'tsdb_base.{quote}Source{quote} IN ({placeholders})')
                params.extend(only_source)
    
            if not_junk is not None:
                conditions.append(f'tsdb_2d.{quote}NOTJUNK{quote} = {placeholder}')
                params.append(True if not_junk else False)
    
            if on_sky is not None:
                mode = 'Observing' if on_sky else 'Calibration'
                conditions.append(f'tsdb_l0.{quote}FIUMODE{quote} = {placeholder}')
                params.append(mode)
    
            if start_date:
                date_str = pd.to_datetime(start_date).strftime("%Y%m%d")
                conditions.append(f'tsdb_base.{quote}datecode{quote} >= {placeholder}')
                params.append(date_str)
    
            if end_date:
                date_str = pd.to_datetime(end_date).strftime("%Y%m%d")
                conditions.append(f'tsdb_base.{quote}datecode{quote} <= {placeholder}')
                params.append(date_str)
    
            if extra_conditions:
                qualified_extra_conditions = []
                for condition in extra_conditions:
                    for keyword, table in kw_table_map.items():
                        condition = re.sub(rf'\\b{keyword}\\b', f'{table}.{quote}{keyword}{quote}', condition)
                    qualified_extra_conditions.append(condition)
    
                extra_clause = f" {extra_conditions_logic} ".join(qualified_extra_conditions)
                conditions.append(f"({extra_clause})")
    
            where_clause = f"WHERE {' AND '.join(conditions)}" if conditions else ""
    
            query = f"""
                SELECT {select_sql}
                FROM {from_clause}
                {' '.join(join_clauses)}
                {where_clause}
            """
    
            if verbose:
                self.logger.debug("SQL Query:")
                self.logger.debug(query)
                self.logger.debug("Params:")
                self.logger.debug(params)
    
            self._execute_sql_command(query, params)
            fetched_data = self.cursor.fetchall()
            col_names = [desc[0] for desc in self.cursor.description]
    
            df = pd.DataFrame(fetched_data, columns=col_names)
    
            # Reorder columns if explicitly requested
            if columns not in (None, '*'):
                final_column_order = [col for col in columns_requested if col in df.columns]
                df = df.sort_values(by='ObsID', ascending=True)
                if 'ObsID' in df.columns and 'ObsID' not in columns_requested:
                    df = df.drop(columns='ObsID')
                df = df[final_column_order]
    
        finally:
            self._close_connection()
    
        return df
        

    @require_role(['admin', 'operations', 'readonly'])
    def ObsIDlist_from_db(self, object_name, start_date=None, end_date=None, not_junk=None):
        """
        Returns a list of ObsIDs for the observations of object_name.

        Args:
            object_name (string) - name of object (e.g., '4614')
            not_junk (True, False, None) using NOTJUNK, select observations that are not Junk (True), Junk (False), or don't care (None)
            start_date (datetime object) - only return observations after start_date
            end_date (datetime object) - only return observations after end_date

        Returns:
            Pandas dataframe of the specified columns matching the constraints.
        """
        # to-do: check if object_name is in the database before trying to create the df
        df = self.dataframe_from_db(['ObsID'], object_like=object_name, 
                                    start_date=start_date, end_date=end_date, 
                                    not_junk=not_junk)
        
        return df['ObsID'].tolist()


def process_file(file_path, now_str,
                 extraction_plan, bool_columns, kw_to_table, keywords_by_table, kw_to_dtype,
                 _extract_kwd_func, _extract_telemetry_func, _extract_rvs_func,
                 get_source_func, get_datecode_func, safe_float):
    """
    Process a single file to extract all relevant keywords based on the extraction plan.
    """    
    
    base_filename = os.path.basename(file_path).replace('.fits', '')
    file_level_paths = {
        'L0': file_path,
        '2D': file_path.replace('L0', '2D').replace('.fits', '_2D.fits'),
        'L1': file_path.replace('L0', 'L1').replace('.fits', '_L1.fits'),
        'L2': file_path.replace('L0', 'L2').replace('.fits', '_L2.fits'),
    }

    extraction_results = {}

    for tbl, plan in extraction_plan.items():
        file_level = plan['file_level']
        extension = plan['extension']
        current_file_path = file_level_paths[file_level]

        keywords = keywords_by_table.get(tbl, [])
        kw_types = {kw: kw_to_dtype[kw] for kw in keywords}

        if not os.path.exists(current_file_path):
            continue

        if tbl == 'tsdb_l0t':
            extracted = _extract_telemetry_func(current_file_path, kw_types)
        elif tbl.startswith('tsdb_l2_') and tbl not in ['tsdb_l2', 'tsdb_l2rv', 'tsdb_l2ccf']:
            extracted = _extract_rvs_func(current_file_path)
        else:
            extracted = _extract_kwd_func(current_file_path, kw_types, extension)

        extraction_results.update(extracted)

    # Mandatory metadata
    extraction_results.update({
        'ObsID': base_filename,
        'datecode': get_datecode_func(base_filename),
        'Source': get_source_func(extraction_results),
        'L0_filename': os.path.basename(file_level_paths['L0']),
        'D2_filename': os.path.basename(file_level_paths['2D']),
        'L1_filename': os.path.basename(file_level_paths['L1']),
        'L2_filename': os.path.basename(file_level_paths['L2']),
        'L0_header_read_time': now_str,
        'D2_header_read_time': now_str,
        'L1_header_read_time': now_str,
        'L2_header_read_time': now_str
    })

    # Convert boolean columns explicitly
    for col in bool_columns:
        if col in extraction_results:
            val = extraction_results[col]
            extraction_results[col] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']

    # Apply safe_float explicitly to all float columns
    for kw, dtype in kw_to_dtype.items():
        if dtype == 'float' and kw in extraction_results:
            extraction_results[kw] = safe_float(extraction_results[kw])

    return extraction_results


def convert_to_list_if_array(string):
    """
    Convert a string like '["autocal-lfc-all-morn", "autocal-lfc-all-eve"]' to an array.
    """
    # Check if the string starts with '[' and ends with ']'
    if type(string) == 'str':
        if string.startswith('[') and string.endswith(']'):
            try:
                # Attempt to parse the string as JSON
                return json.loads(string)
            except json.JSONDecodeError:
                # The string is not a valid JSON array
                return string
    else:
        # The string does not look like a JSON array
        return string
