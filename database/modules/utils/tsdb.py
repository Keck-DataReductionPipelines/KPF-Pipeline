import os
import re
import glob
import json
import copy
import sqlite3
import hashlib
import psycopg2
import pandas as pd
import numpy as np
import time
import inspect
from functools import wraps
from astropy.io import fits
from astropy.time import Time
from astropy.table import Table
from astropy.coordinates import Angle
from tqdm import tqdm
from tqdm.notebook import tqdm_notebook
from datetime import datetime
from functools import partial
from IPython.display import display, HTML
from concurrent.futures import ProcessPoolExecutor

from kpfpipe.models.level1 import KPF1
from kpfpipe.logger import start_logger
from modules.Utils.utils import DummyLogger
from modules.Utils.kpf_parse import get_datecode

DEFAULT_CFG_PATH = 'database/modules/utils/tsdb.cfg'
        
def safe_float(value):
    try:
        return float(value)
    except (ValueError, TypeError):
        return None  # or np.nan

class TSDB:
    """
    Description:
        The TSDB (Time Series DataBase) class facilitates management, ingestion, 
        querying, and reporting of time series data associated with Keck Planet 
        Finder (KPF) observations. It supports interactions with both SQLite3 
        and PostgreSQL backends, providing flexibility for local or server-based 
        deployments.
    
        TSDB is designed to handle diverse data products generated by KPF, 
        including primary header keywords and telemetry from L0 (raw), 2D (two-
        dimensional calibrated), L1 (one-dimensional extracted), and L2 (radial 
        velocity processed) data levels. It extracts header information, 
        telemetry values, radial velocity (RV) measurements, and cross-
        correlation function (CCF) data, storing them efficiently in a 
        structured relational database schema.
    
        Core functionalities include:
        - Database initialization and schema management
        - Dynamic ingestion of observational metadata and telemetry from FITS 
          files
        - Efficient batch processing with multiprocessing support
        - Robust data querying capabilities via user-friendly methods
        - Structured metadata handling using configurable CSV mappings
    
        The database schema and metadata mappings are configured via external 
        CSV files, enabling flexible adjustments without code modifications. 
        This facilitates easy expansion and adaptation to evolving data 
        requirements and observational products.
        
        The database credentials and parameters are stored in the environment 
        variables: TSDBSERVER, TSDBPORT, TSDBNAME, TSDBUSER, TSDBPASS.  
        In a the standard Docker environment, the first three are set by the 
        Docker configuration and TSDBUSER and TSDBPASS are set by environment 
        variables outside of Docker (KPFPIPE_TSDB_USER and KPFPIPE_TSDB_PASS).

    Arguments:
        backend (str):
            Specifies the type of database backend to use ('sqlite' for local 
            file-based storage or 'psql' for PostgreSQL).
        db_path (str):
            Path to the SQLite3 database file. Ignored if backend is PostgreSQL.
        base_dir (str):
            Base directory containing KPF Level 0 (L0) observational data 
            directories.
        credentials (dictionary or None, optional): if set, values for any 
            dictionary keywords (TSDBPORT, TSDBNAME, TSDBSERVER ,TSDBUSER, 
            TSDBPASS) are used instead of environment variables and/or defaults.
            For example, a credentials dictionary is:
                credentials = {"TSDBUSER": 'myuser', "TSDBPASS": 'mypass'}
        logger (logging.Logger or None):
            Logger object for capturing messages, warnings, and errors. If 
            None, a DummyLogger with formatted print statements is used.
        verbose (bool):
            Controls verbosity level of logging output.
    
    Attributes:
        backend (str):
            Database backend in use ('sqlite' or 'psql').
        db_path (str):
            Filesystem path to the SQLite database.
        base_dir (str):
            Root directory for observational data storage.
        tables (dict):
            Dictionary mapping database tables to their CSV configurations.
        extraction_plan (dict):
            Structured plan detailing the file level and FITS extension from 
            which to extract keywords for each database table.
        metadata_entries (list):
            Aggregated list of metadata entries extracted from keyword CSV files.
        metadata_rows (list):
            Cached list of metadata entries loaded directly from the database 
            metadata table.
        bool_columns (set):
            Set of database columns identified as boolean, ensuring proper data 
            typing.
    
    Related Command-Line Scripts:
        - 'ingest_dates_kpf_tsdb.py':
            Ingest observational data for specified date ranges into the TSDB 
            database.
        - 'ingest_watch_kpf_tsdb.py':
            Continuously watch directories for new observational data and 
            automatically ingest them.
    
    To-do:
        - Include derived temperature derivatives and other computed columns.
        - Introduce separate database management for calibration master files.
        - Use config file for backend.  Possibly check the config for credentials.
    
    """
    def __init__(self, 
                 backend='sqlite', db_path='kpf_ts.db', base_dir='/data/L0', 
                 credentials=None, logger=None, verbose=False):

        self.logger = logger if logger is not None else DummyLogger()
        self.logger.info('Starting KPF_TSDB')
        self.verbose = verbose
        
        if self.is_notebook():
            self.tqdm = tqdm_notebook
            self.logger.info('Jupyter Notebook environment detected.')
        else:
            self.tqdm = tqdm

        self.base_dir = base_dir
        self.backend = backend 
        self.logger.info(f'Base data directory: {self.base_dir}')
        self.logger.info(f'Backend: {backend}')
        if self.backend != 'sqlite' and self.backend != 'psql':
            self.logger.info("Invalid entry for backend.  Must be 'sqlite' or 'psql'.")
            return
        self.conn = None
        self.cursor = None

        # Get database parameters
        if self.backend == 'sqlite':
            self.db_path = db_path
            self.logger.info('Path of database file: ' + os.path.abspath(self.db_path))
        elif backend == 'psql':
            creds = credentials if isinstance(credentials, dict) else {}
            # ----- TSDBPORT ------------------------------------------------------------
            if 'TSDBPORT' in creds and creds['TSDBPORT'] is not None:
                self.dbport = creds['TSDBPORT']
                self.logger.info("Using TSDBPORT from credentials dictionary.")
            else:
                self.dbport = os.getenv('TSDBPORT') or '6127'
                if os.getenv('TSDBPORT') is None:
                    self.logger.info("Environment variable 'TSDBPORT' not found; using default port 6127.")
            # ----- TSDBNAME ------------------------------------------------------------
            if 'TSDBNAME' in creds and creds['TSDBNAME'] is not None:
                self.dbname = creds['TSDBNAME']
                self.logger.info("Using TSDBNAME from credentials dictionary.")
            else:
                self.dbname = os.getenv('TSDBNAME') or 'timeseriesopsdb'
                if os.getenv('TSDBNAME') is None:
                    self.logger.info("Environment variable 'TSDBNAME' not found; using default name 'timeseriesopsdb'.")
            # ----- TSDBSERVER ----------------------------------------------------------
            if 'TSDBSERVER' in creds and creds['TSDBSERVER'] is not None:
                self.dbserver = creds['TSDBSERVER']
                self.logger.info("Using TSDBSERVER from credentials dictionary.")
            else:
                self.dbserver = os.getenv('TSDBSERVER') or '127.0.0.1'
                if os.getenv('TSDBSERVER') is None:
                    self.logger.info("Environment variable 'TSDBSERVER' not found; using default server '127.0.0.1'.")
            # ----- TSDBUSER ------------------------------------------------------------
            if 'TSDBUSER' in creds and creds['TSDBUSER'] is not None:
                self.dbuser = creds['TSDBUSER']
                self.logger.info("Using TSDBUSER from credentials dictionary.")
            else:
                self.dbuser = os.getenv('TSDBUSER')
                if os.getenv('TSDBUSER') is None:
                    self.logger.error("Environment variable 'TSDBUSER' not found. No default value available. Many methods in this class won't work.")
            # ----- TSDBPASS ------------------------------------------------------------
            if 'TSDBPASS' in creds and creds['TSDBPASS'] is not None:
                self.dbpass = creds['TSDBPASS']
                self.logger.info("Using TSDBPASS from credentials dictionary.")
            else:
                self.dbpass = os.getenv('TSDBPASS')
                if os.getenv('TSDBPASS') is None:
                    self.logger.error("Environment variable 'TSDBPASS' not found. No default value available. Many methods in this class won't work.")

            self.logger.info('PSQL server: ' + str(self.dbserver))
            self.logger.info('PSQL username: ' + str(self.dbuser))
            self.user_role = self.get_user_role()
            self.logger.info('PSQL user role: ' + self.user_role)

        # Paths
        self.keyword_base_path = '/code/KPF-Pipeline/static/tsdb_tables'
        self.tables_metadata_filepath = os.path.join(self.keyword_base_path, 'tables_metadata.csv')
        self.indexed_csv_path = os.path.join(self.keyword_base_path, 'indexed_columns.csv')
        
        # Read list of tables and columns from files
        self.tables_metadata_df = pd.read_csv(self.tables_metadata_filepath).fillna('')
        tables = {}
        for _, row in self.tables_metadata_df.iterrows():
            table_name = row['table_name']
            tables[table_name] = {
                'csv': row['csv'] if row['csv'] else None,
                'label': row['label'] if row['label'] else None,
                'file_level': row['file_level'] if row['file_level'] else None,
                'extension': row['extension'] if row['extension'] else None,
                'comment': row['comment'] if row['comment'] else None
            }
        self.tables = tables

        # Dynamically build extraction plan
        self.extraction_plan = {
            table_name: {
                'file_level': details['file_level'],
                'extension': details['extension']
            }
            for table_name, details in self.tables.items()
            if details['file_level'] is not None and details['extension'] is not None
        }

# I don't think this is needed any more, but am keeping it during testing.  6/6/2025        
#        # Initialize metadata entries first
#        self._init_metadata_entries()

        # Create metadata table or use existing one
        metadata_table = 'tsdb_metadata'
        if not self.check_if_table_exists(tablename=metadata_table):
            self.logger.info("Metadata table does not exist.  Attempting to create.")
            self._create_metadata_table()
        else:
            self.logger.info("Metadata table exists.")
        self._read_metadata_table()
        self._set_boolean_columns()
        self.kw_to_table = {keyword: table_name for keyword, _, table_name in self.metadata_rows}
        self.kw_to_dtype = {keyword: datatype   for keyword, datatype, _   in self.metadata_rows}
        self.keywords_by_table = {}
        for keyword, table in self.kw_to_table.items():
            self.keywords_by_table.setdefault(table, []).append(keyword)

        # Create the data tables using metadata
        primary_table = 'tsdb_base'
        if not self.check_if_table_exists(tablename=primary_table):
            self.logger.info("Data tables do not exist.  Attempting to create.")
            self._create_data_tables()
        else:
            self.logger.info("Data tables exist.")


    def require_role(allowed_roles=None):
        """
        Decorator that restricts method execution based on backend and user roles.
    
        Args:
            allowed_roles (list or None): List of roles that have access.
                - If None, only sqlite backend is allowed.
                - If a list, psql backend must match one of the roles in the list.
        """
        def decorator(func):
            @wraps(func)
            def wrapper(self, *args, **kwargs):
                if self.backend == 'sqlite':
                    # SQLite backend always allowed if allowed_roles is None or explicitly allowed.
                    return func(self, *args, **kwargs)
    
                elif self.backend == 'psql':
                    if not hasattr(self, 'user_role'):
                        self.user_role = self.get_user_role()
    
                    if allowed_roles and self.user_role in allowed_roles:
                        return func(self, *args, **kwargs)
                    else:
                        allowed_str = ', '.join(allowed_roles) if allowed_roles else 'No roles'
                        raise PermissionError(
                            f"Method '{method_name}' not allowed for role '{self.user_role}'. "
                            f"Allowed roles: {allowed_str}"
                        )
    
                else:
                    raise ValueError(f"Unsupported backend: {self.backend}")
    
            return wrapper
        return decorator


    def _open_connection(self):
        """
        Establishes a connection to the database and initializes the cursor.
    
        This method supports both SQLite and PostgreSQL backends. It attempts 
        multiple connection retries when using PostgreSQL to handle transient 
        connection issues.
    
        Behavior by backend:
            - SQLite:
                Opens a connection to the database file specified by 
                `self.db_path`.
            - PostgreSQL:
                Attempts to connect to the PostgreSQL database using parameters 
                defined by:
                    - `self.dbserver`
                    - `self.dbname`
                    - `self.dbport`
                    - `self.dbuser`
                    - `self.dbpass`
                Retries up to three times if the connection fails initially, 
                waiting 10 seconds between attempts.
        """

        if self.backend == 'sqlite':
            self.conn = sqlite3.connect(self.db_path)
            self.cursor = self.conn.cursor()
        elif self.backend == 'psql':
            try:
                db_fail = True
                n_attempts = 3
                for i in range(n_attempts):
                    try: 
                        self.conn = psycopg2.connect(
                            host=self.dbserver,
                            database=self.dbname,
                            port=self.dbport,
                            user=self.dbuser,
                            password=self.dbpass
                        )
                        db_fail = False
                        break
                    except:
                        self.logger.info("Could not connect to database, retrying...")
                        db_fail = True
                        time.sleep(10)
                if self.conn == None:
                    self.logger.error('Database connection not established.  self.conn = None.')
                self.cursor = self.conn.cursor()
            except Exception as e:
                self.logger.error(f"Failed to connect to PostgreSQL: {e}")
                raise
                
    
    def _close_connection(self):
        """
        Commits any pending database transactions and safely closes the database 
        connection and cursor.
    
        Behavior by backend:
            - SQLite:
                Commits all changes to the database file and closes the 
                connection.
            - PostgreSQL:
                Commits transactions to the PostgreSQL database and closes the 
                connection.
    
        After executing this method, the internal connection (`self.conn`) and 
        cursor (`self.cursor`) are reset to None to indicate that no active 
        connection exists.
        """
        if self.conn:
            if self.backend == 'sqlite':
                self.conn.commit()
            elif self.backend == 'psql':
                self.conn.commit()
            self.cursor.close()
            self.conn.close()
            self.conn = None
            self.cursor = None

    
    def _execute_sql_command(self, command, params=None, fetch=False):
        """
        Execute an SQL command using the currently opened database connection.
        
        This method supports both SQLite and PostgreSQL backends, automatically 
        adjusting placeholder syntax as needed. It optionally returns fetched 
        data and logs detailed debugging information when verbose mode is 
        enabled.
    
        Args:
            command (str): 
                The SQL query or command to execute. Use '?' placeholders for 
                parameters.
            params (tuple or list, optional): 
                Parameters to bind to the SQL command. Defaults to None.
            fetch (bool, optional): 
                If True, fetches and returns all results from the executed query.
                Defaults to False.
    
        Returns:
            list of tuples:
                Result set returned by `cursor.fetchall()` if fetch is True; 
                otherwise, None.
        """
        if self.cursor is None:
            raise RuntimeError("Database connection is not open.")
    
        try:
            if params:
                if self.backend == 'sqlite':
                    self.cursor.execute(command, params)
                    if self.verbose:
                        self.logger.debug(f'query: {command}')
                elif self.backend == 'psql':
                    self.cursor.execute(command.replace('?', '%s'), params)
                    if self.verbose:
                        self.logger.debug(f'query: {command}')
            else:
                if self.verbose:
                    self.logger.debug(f'query: {command}')
                self.cursor.execute(command)
    
            if fetch:
                response = self.cursor.fetchall()
                if self.verbose:
                    self.logger.debug(f'response: {response}')
                return response
        except Exception as e:
            self.logger.error(f"SQL Execution error: {e}\nCommand: {command}\nParams: {params}")
            raise


    def get_user_role(self):
        """
        Determine and return the database user's role based on schema-level 
        permissions.
    
        This method queries PostgreSQL schema privileges for the current user 
        and classifies the user's role into one of four categories based on 
        their permissions:
    
        - 'admin': User has SUPERUSER privileges, providing full administrative 
          access.
        - 'operations': User has both CREATE and USAGE privileges on the 
          database schema, suitable for typical operational tasks, including 
          data ingestion and table management.
        - 'readonly': User has only USAGE privileges, restricting their actions 
          to read-only database queries.
        - 'none': User lacks USAGE, CREATE, and SUPERUSER privileges, indicating 
          no effective permissions to interact with the schema.
    
        Returns:
            str: The user's role classification ('admin', 'operations', 
                 'readonly', or 'none').
        """
        if self.backend != 'psql':
            raise ValueError("get_user_role only supported with PostgreSQL backend")
    
        schema_perms = self._check_user_schema_permissions()
    
        if schema_perms['SUPERUSER']:
            return 'admin'
        elif schema_perms['CREATE'] and schema_perms['USAGE']:
            return 'operations'
        elif schema_perms['USAGE']:
            return 'readonly'
        else:
            return 'none'


    def _check_user_schema_permissions(self, schema_name='public'):
        """
        Checks schema-level permissions (usage and create) independently of 
        any table.
    
        Args:
            schema_name (str): Schema name to check permissions for.
    
        Returns:
            dict: User permissions indicating access capabilities.
        """
        permissions = {
            'USAGE': False,
            'CREATE': False,
            'SUPERUSER': False
        }
    
        self._open_connection()
        try:
            # Check schema-level privileges directly
            self.cursor.execute("""
                SELECT
                    has_schema_privilege(current_user, %s, 'USAGE'),
                    has_schema_privilege(current_user, %s, 'CREATE'),
                    rolsuper
                FROM pg_roles
                WHERE rolname = current_user;
            """, (schema_name, schema_name))
    
            result = self.cursor.fetchone()
            if result:
                permissions['USAGE'], permissions['CREATE'], permissions['SUPERUSER'] = result
    
        except psycopg2.Error as e:
            self.logger.error(f"Error checking schema permissions: {e}")
    
        finally:
            self._close_connection()
    
        return permissions

# I think this isn't used any more.  AWH 6/6/2025
#    @require_role(['admin', 'operations'])
#    def print_summary_all_tables(self):
#        """
#        Prints a summary of all tables in the database (not just the intended tables), 
#        including the number of rows and columns.
#        """
#
#        self._open_connection()
#        try:
#            if self.backend == 'sqlite':
#                self._execute_sql_command("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;")
#                tables = [row[0] for row in self.cursor.fetchall()]
#    
#                print(f"{'Table Name':<20} {'Columns':>7} {'Rows':>10}")
#                print("-" * 40)
#    
#                for tbl in tables:
#                    self._execute_sql_command(f"PRAGMA table_info({tbl});")
#                    columns = len(self.cursor.fetchall())
#    
#                    self._execute_sql_command(f"SELECT COUNT(*) FROM {tbl};")
#                    rows = self.cursor.fetchone()[0]
#    
#                    print(f"{tbl:<20} {columns:>7} {rows:>10}")
#    
#            elif self.backend == 'psql':
#                self._execute_sql_command("""
#                     SELECT tablename FROM pg_catalog.pg_tables
#                     WHERE schemaname='public' ORDER BY tablename;
#                """)
#                tables = [row[0] for row in self.cursor.fetchall()]
#         
#                print(f"{'Table Name':<20} {'Columns':>7} {'Rows':>10}")
#                print("-" * 40)
#         
#                for tbl in tables:
#                    self._execute_sql_command(f"""
#                        SELECT COUNT(*) FROM information_schema.columns
#                        WHERE table_name = '{tbl}';
#                    """)
#                    columns = self.cursor.fetchone()[0]
#         
#                    self._execute_sql_command(f"SELECT COUNT(*) FROM {tbl};")
#                    rows = self.cursor.fetchone()[0]
#         
#                    print(f"{tbl:<20} {columns:>7} {rows:>10}")
#        finally:
#            self._close_connection()


    @require_role(['admin', 'operations'])
    def drop_tables(self, tables = 'all'):
        """
        Start over on the database by dropping all data and metadata tables.
        """
        if tables == 'all':
            tables = self.tables
        
        self._open_connection()
        try:
            for tbl in tables:
                self._execute_sql_command(f"DROP TABLE IF EXISTS {tbl}")
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def unlock_db(self):
        """
        Remove the -wal and -shm lock files for SQLite databases.
        For PostgreSQL, prints a message indicating that unlocking is not 
        required.
        """
        if self.backend == 'sqlite':
            wal_file = f"{self.db_path}-wal"
            shm_file = f"{self.db_path}-shm"
    
            if os.path.exists(wal_file):
                os.remove(wal_file)
                self.logger.info(f"File removed: {wal_file}")
            if os.path.exists(shm_file):
                os.remove(shm_file)
                self.logger.info(f"File removed: {shm_file}")
        elif self.backend == 'psql':
            self.logger.info("unlock_db() is not applicable for PostgreSQL databases.")


    @require_role(['admin', 'operations', 'readonly'])
    def check_if_table_exists(self, tablename=None):
        """
        Check if the specified table exists in the database.
        """
        if tablename is None:
            self.logger.info('check_if_table_exists: tablename not specified.')
            return False
    
        self._open_connection()
        try:
            if self.backend == 'sqlite':
                query = "SELECT name FROM sqlite_master WHERE type='table' AND name=?;"
            elif self.backend == 'psql':
                query = "SELECT table_name FROM information_schema.tables WHERE table_schema='public' AND table_name=%s;"
    
            result = self._execute_sql_command(query, params=(tablename,), fetch=True)
            return len(result) > 0
    
        except Exception as e:
            self.logger.error(f"Error checking table existence: {e}")
            return False
        finally:
            self._close_connection()

# I think this no longer needed.  Keeping it for tests just in case.  AWH 6/6/25
#    def _init_metadata_entries(self):
#        """
#        Load and combine all keyword metadata entries from CSV files into self.metadata_entries.
#        """
#        dfs = []
#        tables_metadata_df = pd.read_csv(self.csv_filepath).fillna('')
#    
#        for _, row in tables_metadata_df.iterrows():
#            csv_filename = row['csv']
#            label = row['label']
#            table_name = row['table_name']
#            
#            if not csv_filename:
#                continue  # skip if CSV not specified
#    
#            csv_path = os.path.join(self.keyword_base_path, csv_filename)
#            df = pd.read_csv(csv_path, delimiter='|', dtype=str).fillna('')
#            df['source'] = label
#            df['table_name'] = table_name
#            df['csv_path'] = csv_path
#            df.rename(columns={'unit': 'units'}, inplace=True)
#    
#            dfs.append(df[['keyword', 'datatype', 'units', 'description', 'source', 'table_name', 'csv_path']])
#    
#        # Combine all dataframes into one
#        df_all = pd.concat(dfs, ignore_index=True)
#        df_all.drop_duplicates(subset='keyword', inplace=True)
#    
#        self.metadata_entries = df_all.to_dict(orient='records')
#
#   
    @require_role(['admin', 'operations', 'readonly'])
    def print_db_status(self):
        """
        Prints a formatted summary table of the database status for each table,
        ensuring tables exist first, and handling both SQLite and PostgreSQL.
        """
        tables = [table for table in self.tables if table != 'tsdb_metadata']
    
        self._open_connection()
    
        summary_data = []
    
        try:
            for table in tables:
                # Verify table existence first
                table_exists = False
    
                if self.backend == 'sqlite':
                    self._execute_sql_command(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name=?;",
                        params=(table,)
                    )
                    table_exists = bool(self.cursor.fetchone())
                elif self.backend == 'psql':
                    self._execute_sql_command(
                        """
                        SELECT EXISTS (
                            SELECT 1 FROM information_schema.tables
                            WHERE table_name=%s
                        );
                        """, params=(table,)
                    )
                    table_exists = self.cursor.fetchone()[0]
    
                if not table_exists:
                    self.logger.warning(f"Table '{table}' does not exist; skipping.")
                    continue
    
                self._execute_sql_command(f'SELECT COUNT(*) FROM {table}')
                nrows = self.cursor.fetchone()[0]
    
                if self.backend == 'sqlite':
                    self._execute_sql_command(f'PRAGMA table_info({table})')
                    ncolumns = len(self.cursor.fetchall())
                elif self.backend == 'psql':
                    self._execute_sql_command(
                        """
                        SELECT COUNT(*) FROM information_schema.columns
                        WHERE table_name=%s;
                        """, params=(table,)
                    )
                    ncolumns = self.cursor.fetchone()[0]
    
                summary_data.append((table, ncolumns, nrows))
    
            if not summary_data:
                self.logger.info("No tables exist.")
                return
    
            # Fetch additional stats if 'tsdb_base' exists
            if 'tsdb_base' in [t[0] for t in summary_data]:
                query_time_col = 'L0_header_read_time' if self.backend == 'sqlite' else '"L0_header_read_time"'
                query_datecode_col = 'datecode' if self.backend == 'sqlite' else '"datecode"'
    
                self._execute_sql_command(
                    f'SELECT MAX({query_time_col}) FROM tsdb_base'
                )
                most_recent_read_time = self.cursor.fetchone()[0] or 'N/A'
    
                self._execute_sql_command(
                    f'SELECT MIN({query_datecode_col}), MAX({query_datecode_col}), COUNT(DISTINCT {query_datecode_col}) FROM tsdb_base'
                )
                earliest_datecode, latest_datecode, unique_datecodes_count = self.cursor.fetchone()
    
                earliest_datecode = earliest_datecode or 'N/A'
                latest_datecode = latest_datecode or 'N/A'
                unique_datecodes_count = unique_datecodes_count or 0
            else:
                most_recent_read_time = earliest_datecode = latest_datecode = 'N/A'
                unique_datecodes_count = 0
    
            # Print the summary table
            self.logger.info("Database Table Summary:")
            self.logger.info(f"{'Table':<15} {'Columns':>7} {'Rows':>10}")
            self.logger.info("-" * 35)
            for table, cols, rows in summary_data:
                self.logger.info(f"{table:<15} {cols:>7} {rows:>10}")
    
            # Print additional stats
            self.logger.info(f"Dates: {unique_datecodes_count} days from {earliest_datecode} to {latest_datecode}")
            self.logger.info(f"Last update: {most_recent_read_time}")
    
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def _create_metadata_table(self):
        """
        Create the tsdb_metadata table, tracking whether each column is 
        indexed based on:
            - Explicitly listed columns from a file
            - Columns with descriptions containing 'QC:'.
        """
        indexed_df = pd.read_csv(self.indexed_csv_path)
        indexed_columns = set(indexed_df['column'].tolist())
    
        # Drop existing metadata table if it exists
        self._open_connection()
        try:
            self._execute_sql_command("DROP TABLE IF EXISTS tsdb_metadata")
    
            # Create metadata table with 'indexed' column
            create_sql = """
                CREATE TABLE tsdb_metadata (
                    keyword     TEXT PRIMARY KEY,
                    source      TEXT,
                    datatype    TEXT,
                    units       TEXT,
                    description TEXT,
                    table_name  TEXT,
                    indexed     BOOLEAN
                );
            """
            self._execute_sql_command(create_sql)
    
            # Prepare insert SQL (SQLite and PostgreSQL compatible)
            insert_sql = """
                INSERT INTO tsdb_metadata 
                (keyword, source, datatype, units, description, table_name, indexed)
                VALUES (?, ?, ?, ?, ?, ?, ?);
            """ if self.backend == 'sqlite' else """
                INSERT INTO tsdb_metadata 
                (keyword, source, datatype, units, description, table_name, indexed)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (keyword) DO UPDATE SET
                    source=EXCLUDED.source,
                    datatype=EXCLUDED.datatype,
                    units=EXCLUDED.units,
                    description=EXCLUDED.description,
                    table_name=EXCLUDED.table_name,
                    indexed=EXCLUDED.indexed;
            """
    
            # Load tables metadata
            tables_metadata_df = pd.read_csv(self.tables_metadata_filepath).fillna('')
    
            # Iterate over metadata CSVs and populate entries
            for _, row in tables_metadata_df.iterrows():
                csv_filename = row['csv']
                source = row['label']
                table_name = row['table_name']
    
                if not csv_filename:
                    continue
    
                csv_path = os.path.join(self.keyword_base_path, csv_filename)
                df_keywords = pd.read_csv(csv_path, delimiter='|', dtype=str).fillna('')
    
                for _, keyword_row in df_keywords.iterrows():
                    keyword = keyword_row['keyword']
                    datatype = keyword_row.get('datatype', 'TEXT')
                    units = keyword_row.get('unit', '')
                    description = keyword_row.get('description', '')
    
                    # Determine indexing based on CSV or QC in description
                    is_indexed = (keyword in indexed_columns) or ('QC:' in description)
    
                    self._execute_sql_command(
                        insert_sql,
                        params=(keyword, source, datatype, units, description, table_name, is_indexed)
                    )
    
            self.logger.info("Metadata table created correctly with indexed columns.")
    
        finally:
            self._close_connection()


    @require_role(['admin', 'operations', 'readonly'])
    def _read_metadata_table(self):
        """
        Read the tsdb_metadata table and store it in the attribute 
        self.metadata_table.
        """
        sql_metadata = "SELECT keyword, datatype, table_name FROM tsdb_metadata;"
        self._open_connection()
        self.metadata_rows = self._execute_sql_command(sql_metadata, fetch=True)
        self._close_connection()
        self.logger.info("Metadata table read.")


    @require_role(['admin', 'operations'])
    def _create_data_tables(self):
        """
        Create TSDB data tables from metadata definitions, with ObsID as 
        primary key.
    
        Columns specified in a file and columns whose metadata description 
        contains the substring \"QC:\" are automatically indexed. Indexing is 
        based on column presence in the metadata, which may include multiple 
        tables.
    
        Key Steps:
            1. Loads columns to index from `indexed_columns.csv`.
            2. Retrieves keyword, datatype, table_name, and description from 
               the metadata table.
            3. Dynamically creates each table with ObsID as the primary key.
            4. Creates indices for:
                - Columns explicitly listed in `indexed_columns.csv`.
                - Columns identified by having \"QC:\" in their metadata 
                  descriptions.
            5. Ensures ObsID is indexed in tables other than `tsdb_base`.
            6. Updates the metadata table to reflect indexed columns with an 
               `indexed` boolean flag.
    
        Logs detailed messages indicating index creation successes and ensures 
        database operations are completed safely by properly closing connections.
        """
    
        indexed_df = pd.read_csv(self.indexed_csv_path)
        indexed_columns = set(indexed_df['column'].tolist())
    
        tables = [table for table in self.tables if table != 'tsdb_metadata']
        sql_metadata = "SELECT keyword, datatype, table_name, description FROM tsdb_metadata;"
    
        self._open_connection()
        metadata_rows = self._execute_sql_command(sql_metadata, fetch=True)
    
        columns_by_table = {tbl: [] for tbl in tables}
        tables_by_column = {}
        qc_columns = set()
    
        for keyword, dtype, table_name, description in metadata_rows:
            if table_name not in columns_by_table or table_name is None:
                continue
            sql_type = self._map_data_type_to_sql(dtype)
            columns_by_table[table_name].append((keyword, sql_type))
            tables_by_column.setdefault(keyword, set()).add(table_name)
    
            if description and "QC:" in description:
                qc_columns.add(keyword)
    
        try:
            for tbl, cols in columns_by_table.items():
                col_defs = ['"ObsID" TEXT PRIMARY KEY']
                col_defs += [f'"{kw}" {sql_type}' for kw, sql_type in cols if kw.lower() != 'obsid']
                col_defs_sql = ", ".join(col_defs)
    
                create_table_sql = f"CREATE TABLE IF NOT EXISTS {tbl} ({col_defs_sql});"
                self._execute_sql_command(create_table_sql)
    
            all_indexed_columns = indexed_columns.union(qc_columns)
    
            for column in all_indexed_columns:
                tables_to_index = tables_by_column.get(column, set())
    
                for tbl in tables_to_index:
                    if tbl == 'tsdb_base' and column.lower() == 'obsid':
                        continue
    
                    index_name = f"{tbl}_{column}_idx"
                    index_sql = f'CREATE INDEX IF NOT EXISTS "{index_name}" ON {tbl} ("{column}");'
                    self._execute_sql_command(index_sql)
                    if self.verbose:
                        self.logger.debug(f"Created index: {index_name} on table {tbl}({column})")
    
            # Check if the column 'indexed' exists in tsdb_metadata
            self._execute_sql_command("PRAGMA table_info(tsdb_metadata);")
            existing_columns = [row[1] for row in self.cursor.fetchall()]
            
            if 'indexed' not in existing_columns:
                update_indexed_sql = """
                    ALTER TABLE tsdb_metadata ADD COLUMN indexed BOOLEAN DEFAULT FALSE;
                """
                self._execute_sql_command(update_indexed_sql)
    
            for column in all_indexed_columns:
                update_metadata_sql = f"""
                    UPDATE tsdb_metadata SET indexed = TRUE WHERE keyword = ?;
                """
                self._execute_sql_command(update_metadata_sql, params=(column,))
    
        finally:
            self._close_connection()
    
        self.logger.info("Data tables and indices created successfully.")

 
    @require_role(['admin', 'operations'])
    def create_test_table(self, tablename, schema='public'):
        """
        Create a table to test database functionality.
        """
        
        self._open_connection()
    
        try:
            # Specify schema explicitly
            self.cursor.execute(f"CREATE TABLE {schema}.test (id integer);")
            self.conn.commit()
            print(f"CREATE TABLE {schema}.{tablename}")
    
        except psycopg2.Error as e:
            print(f"Error creating table: {e}")
            self.conn.rollback()
    
        finally:
            self._close_connection()
           

    @require_role(['admin', 'operations', 'readonly'])
    def _set_boolean_columns(self):
        """
        Set the self.bool_columns attribute with the names of all database 
        columns that should be treated as booleans, based on the metadata table.
        """
        self._open_connection()
        try:
            self.bool_rows = self._execute_sql_command(
                "SELECT keyword, datatype, table_name FROM tsdb_metadata WHERE datatype = 'bool';",
                fetch=True
            )
            self.bool_columns = {row[0] for row in self.bool_rows}
            if self.verbose:
                self.logger.debug(f"Boolean columns set: {self.bool_columns}")
        finally:
            self._close_connection()


    @require_role(['admin', 'operations'])
    def ingest_one_observation(self, dir_path, L0_filename):
        """
        Ingest metadata and telemetry from a single KPF observation into the 
        database.
    
        This method processes a single observation by extracting data from FITS 
        files associated with different processing levels (L0, 2D, L1, L2). 
        The extracted metadata, telemetry, radial velocities, and related 
        keywords are dynamically mapped and inserted into the corresponding 
        database tables.
    
        Steps:
            1. Determine file paths for associated data products (L0, 2D, L1, 
               L2).
            2. Extract header keywords, telemetry data, and radial velocity 
               measurements based on the predefined extraction plan.
            3. Aggregate extracted data into structured dictionaries keyed by 
               database tables.
            4. Apply necessary data type conversions (e.g., floats, booleans).
            5. Insert or update entries into respective database tables, 
               resolving conflicts by updating existing records if necessary.
    
        Args:
            dir_path (str):
                Filesystem path to the directory containing the L0 FITS file.
            L0_filename (str):
                Filename of the Level 0 (raw) FITS file, typically in the format 
                'ObsID.fits' (e.g., 'KP.20241020.12345.67.fits').
    
        Preconditions:
            - Directory at `dir_path` exists and contains the L0 file and its 
              related products.
            - Database schema and metadata table (`tsdb_metadata`) are properly 
              initialized.
            - The method is executed with adequate database permissions ('admin' 
              or 'operations').
    
        Postconditions:
            - Observation metadata, telemetry, and measurements are ingested 
              into the database.
            - Existing records for the same ObsID are updated if they already 
              exist.
    
        Usage:
            Intended for incremental ingestion, such as manual ingestion or 
            pipeline-triggered events.
    
        Example:
            tsdb.ingest_one_observation('/data/L0/20241020/', 'KP.20241020.12345.67.fits')
        """
        base_filename = L0_filename.split('.fits')[0]
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
        extraction_results = {}
        
        for tbl, plan in self.extraction_plan.items():
            file_level = plan['file_level']
            ext = plan['extension']
            file_path = (
                f"{dir_path.replace('L0', file_level)}/{base_filename}_{file_level}.fits"
                if file_level != 'L0'
                else f"{dir_path}/{base_filename}.fits"
            )
     
            keywords = self.keywords_by_table.get(tbl, [])
    
            if tbl == 'tsdb_l0t':
                extracted = self._extract_telemetry(file_path, {kw: self.kw_to_dtype[kw] for kw in keywords})
            elif tbl.startswith('tsdb_l2_') and tbl not in ['tsdb_l2', 'tsdb_l2_rv', 'tsdb_l2_ccf']:
                extracted = self._extract_rvs(file_path)
            else:
                kw_types = {kw: self.kw_to_dtype[kw] for kw in keywords}
                extracted = self._extract_kwd(file_path, kw_types, ext)
    
            extraction_results.update(extracted)
    
        # the Angle code was crashing
        def safe_angle(value, unit, default=0.0):
            try:
                return Angle(value, unit=unit).degree
            except Exception:
                return default

        extraction_results.update({
            'ObsID': base_filename,
            'datecode': get_datecode(base_filename),
            'Source': self.get_source(extraction_results),
            'L0_filename': f"{base_filename}.fits",
            'D2_filename': f"{base_filename}_2D.fits",
            'L1_filename': f"{base_filename}_L1.fits",
            'L2_filename': f"{base_filename}_L2.fits",
            'L0_header_read_time': now_str,
            'D2_header_read_time': now_str,
            'L1_header_read_time': now_str,
            'L2_header_read_time': now_str,
            'RA_deg': safe_angle(extraction_results.get('RA'), unit='hourangle'),
            'DEC_deg': safe_angle(extraction_results.get('DEC'), unit='deg')
        })
    
        for kw, dtype in self.kw_to_dtype.items():
            if dtype == 'float' and kw in extraction_results:
                extraction_results[kw] = safe_float(extraction_results[kw])
    
        self._open_connection()
        try:
            table_data = {}
            for kw, value in extraction_results.items():
                table_name = self.kw_to_table.get(kw)
                if table_name:
                    table_data.setdefault(table_name, {})[kw] = value
    
            for tbl, data in table_data.items():
                for kw in data:
                    if kw in self.bool_columns:
                        val = data[kw]
                        data[kw] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']
                data['ObsID'] = base_filename
    
            for tbl, data in table_data.items():
                columns = ', '.join(f'"{col}"' for col in data)
                placeholders = ', '.join(['?'] * len(data)) if self.backend == 'sqlite' else ', '.join(['%s'] * len(data))
    
                if self.backend == 'sqlite':
                    insert_query = f'INSERT OR REPLACE INTO {tbl} ({columns}) VALUES ({placeholders})'
                else:
                    updates = ', '.join(f'"{col}"=EXCLUDED."{col}"' for col in data if col != 'ObsID')
                    insert_query = (
                        f'INSERT INTO {tbl} ({columns}) VALUES ({placeholders}) '
                        f'ON CONFLICT ("ObsID") DO UPDATE SET {updates}'
                    )
    
                self._execute_sql_command(insert_query, params=tuple(data.values()))
        finally:
            self._close_connection()
    
        self.logger.info(f"Ingested observation: {base_filename}")


    @require_role(['admin', 'operations'])
    def ingest_dates_to_db(self, start_date_str, end_date_str, batch_size=10000, reverse=False, force_ingest=False, quiet=False):
        """
        Ingest observational data from a specified date range into the database.
    
        This method systematically ingests metadata and telemetry from Keck 
        Planet Finder (KPF) observations for each date within the provided start 
        and end dates. It processes Level 0 FITS files and their associated data 
        products (2D, L1, and L2), extracting relevant information according to 
        the extraction plan. Observations are processed in batches to optimize 
        performance and database transactions.
    
        Workflow:
            1. Identify and sort directories corresponding to observation dates.
            2. Iterate through each observation date, processing FITS files in 
               batches.
            3. Optionally skip re-ingesting observations unless files have been 
               modified after the last database update.
            4. Perform extraction, aggregation, and insertion of data into the 
               database using multiprocessing for efficiency.
    
        Args:
            start_date_str (str or datetime):
                Start date for ingestion (inclusive), formatted as 'YYYYMMDD' or 
                as a datetime object.
            end_date_str (str or datetime):
                End date for ingestion (inclusive), formatted as 'YYYYMMDD' or 
                as a datetime object.
            batch_size (int, optional, default=10000):
                Maximum number of observations processed per database transaction 
                batch.
            reverse (bool, optional, default=False):
                If True, processes observation dates in reverse chronological 
                order.
            force_ingest (bool, optional, default=False):
                If True, ingests observations regardless of their modification 
                timestamps in the database.
            quiet (bool, optional, default=False):
                If True, suppresses progress bar and reduces logging verbosity.
    
        Preconditions:
            - Database tables and schema are properly initialized.
            - Directories containing KPF observation files (`self.base_dir`) 
              exist and follow the expected structure.
            - User has sufficient database privileges ('admin' or 'operations').
    
        Postconditions:
            - Database is populated or updated with observational data within 
              the specified date range.
    
        Usage Scenario:
            - Routine daily ingestion or re-ingestion of observational data.
            - Batch updating of database following pipeline reprocessing events.
    
        Example:
            tsdb.ingest_dates_to_db('20241001', '20241031', batch_size=500, reverse=True, force_ingest=True)
        """

        # Convert input dates to strings if necessary
        if isinstance(start_date_str, datetime):
            start_date_str = start_date_str.strftime("%Y%m%d")
        if isinstance(end_date_str, datetime):
            end_date_str = end_date_str.strftime("%Y%m%d")
        
        if not quiet:
            self.logger.info("Adding to database between " + start_date_str + " and " + end_date_str)
        dir_paths = glob.glob(f"{self.base_dir}/????????")
        sorted_dir_paths = sorted(dir_paths, key=lambda x: int(os.path.basename(x)), reverse=start_date_str > end_date_str)
        filtered_dir_paths = [
            dir_path for dir_path in sorted_dir_paths
            if start_date_str <= os.path.basename(dir_path) <= end_date_str
        ]
        if len(filtered_dir_paths) > 0:
            # Reverse dates if the reverse flag is set
            if reverse:
                filtered_dir_paths.reverse()
            
            # Iterate over date directories
            t1 = self.tqdm(filtered_dir_paths, desc=(filtered_dir_paths[0]).split('/')[-1], disable=quiet)
            for dir_path in t1:
                t1.set_description(dir_path.split('/')[-1])
                t1.refresh() 
                t2 = self.tqdm(os.listdir(dir_path), desc=f'Files', leave=False, disable=quiet)
                batch = []
                for L0_filename in t2:
                    if L0_filename.endswith(".fits"):
                        file_path = os.path.join(dir_path, L0_filename)
                        batch.append(file_path)
                        if len(batch) >= batch_size:
                            self.ingest_batch_observations(batch, force_ingest=force_ingest)
                            batch = []
                if batch:
                    self.ingest_batch_observations(batch, force_ingest=force_ingest)

        if not quiet:
            self.logger.info(f"Files for {len(filtered_dir_paths)} days ingested/checked")


    @require_role(['admin', 'operations'])
    def ingest_batch_observations(self, batch, force_ingest=False):
        """
        Ingest a batch of observational data files into the database efficiently 
        using parallel processing.
    
        This method handles multiple Keck Planet Finder (KPF) observational 
        files simultaneously, extracting metadata, telemetry, radial velocity 
        (RV) measurements, and other relevant data from Level 0 (L0), 
        Level 2D (2D), Level 1 (L1), and Level 2 (L2) FITS files. It employs 
        multiprocessing to accelerate data extraction and ingestion processes, 
        significantly enhancing performance during batch operations.
    
        Workflow:
            1. Optionally filters the batch based on file modification times 
               compared to database entries, unless `force_ingest` is set to 
               True.
            2. Utilizes multiprocessing to concurrently extract required 
               information from each file according to a predefined extraction 
               plan.
            3. Aggregates the extracted data into structured dictionaries mapped 
               to appropriate database tables.
            4. Inserts or updates database records efficiently in bulk, handling 
               conflicts as necessary.
    
        Args:
            batch (list of str):
                List of file paths corresponding to KPF observational data 
                (primarily L0 FITS files) to be ingested.
            force_ingest (bool, optional, default=False):
                If True, ingests all files in the batch irrespective of their 
                modification status relative to the database.
    
        Preconditions:
            - Database schema and tables are properly initialized.
            - Files listed in `batch` exist and are accessible.
            - User role has sufficient privileges (roles: 'admin', 'operations').
    
        Postconditions:
            - Database tables are populated or updated with data extracted from 
              the provided observational files.
            - Data integrity and consistency are maintained via proper conflict 
              handling and data type conversions.
    
        Performance:
            - Parallelized processing leveraging available CPU cores, optimized 
              for batches up to thousands of files.
    
        Raises:
            Exception:
                Logs and raises exceptions encountered during data extraction or 
                database operations, providing detailed information for
                debugging.
    
        Usage Scenario:
            - Ideal for ingesting or updating database entries following bulk 
              reprocessing of KPF data.
            - Suitable for scheduled ingestion tasks involving large quantities 
              of observational data.
    
        Example:
            file_batch = ['/data/L0/20241001/KP.20241001.12345.67.fits', 
                          '/data/L0/20241001/KP.20241001.12346.68.fits']
            tsdb.ingest_batch_observations(file_batch, force_ingest=True)
        """

        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
        # Filter updated files
        updated_batch = batch if force_ingest else [
            file_path for file_path in batch if self._is_any_file_updated(file_path)
        ]
        if not updated_batch:
            if self.verbose:
                self.logger.info("No new or updated files found in batch.")
            return
    
        extraction_args = {
            'now_str': now_str,
            'extraction_plan': self.extraction_plan,
            'bool_columns': self.bool_columns,
            'kw_to_table': self.kw_to_table,
            'keywords_by_table': self.keywords_by_table,
            'kw_to_dtype': self.kw_to_dtype,
            '_extract_kwd_func': self._extract_kwd,
            '_extract_telemetry_func': self._extract_telemetry,
            '_extract_rvs_func': self._extract_rvs,
            'get_source_func': self.get_source,
            'get_datecode_func': get_datecode
        }
    
        partial_process_file = partial(
            process_file, 
            safe_float=safe_float, 
            **extraction_args
        )
    
        max_workers = min(len(updated_batch), 20, os.cpu_count())
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(partial_process_file, updated_batch))
    
        valid_results = [res for res in results if res]
    
        if not valid_results:
            self.logger.warning("No valid results extracted from batch files.")
            return
    
        self._open_connection()
        try:
            table_data = {}
            for obs_data in valid_results:
                obsid = obs_data['ObsID']
                for kw, value in obs_data.items():
                    table = self.kw_to_table.get(kw)
                    if table:
                        table_data.setdefault(table, []).append((obsid, kw, value))
    
            for tbl, rows in table_data.items():
                structured_data = {}
                for obsid, kw, val in rows:
                    structured_data.setdefault(obsid, {'ObsID': obsid})[kw] = val
    
                # Boolean handling
                for data in structured_data.values():
                    for kw in data:
                        if kw in self.bool_columns:
                            val = data[kw]
                            data[kw] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']
    
                columns = list(next(iter(structured_data.values())).keys())
                col_str = ', '.join(f'"{col}"' for col in columns)
    
                if self.backend == 'sqlite':
                    placeholder_char = '?'
                    placeholders = ', '.join([placeholder_char] * len(columns))
                    insert_query = f'INSERT OR REPLACE INTO {tbl} ({col_str}) VALUES ({placeholders})'
                else:
                    placeholder_char = '%s'
                    placeholders = ', '.join([placeholder_char] * len(columns))
                    updates = ', '.join(f'"{col}" = EXCLUDED."{col}"' for col in columns if col != 'ObsID')
                    conflict_clause = (
                        f'ON CONFLICT ("ObsID") DO UPDATE SET {updates}' if updates else 'ON CONFLICT ("ObsID") DO NOTHING'
                    )
                    insert_query = f'INSERT INTO {tbl} ({col_str}) VALUES ({placeholders}) {conflict_clause}'
    
                data_tuples = [tuple(data[col] for col in columns) for data in structured_data.values()]
    
                try:
                    self.cursor.executemany(insert_query, data_tuples)
                except Exception as e:
                    self.logger.error(f"Bulk insert error in table {tbl}: {e}")
                    for data_tuple in data_tuples:
                        try:
                            self._execute_sql_command(insert_query, params=data_tuple)
                        except Exception as single_e:
                            problematic_obsid = data_tuple[columns.index('ObsID')]
                            self.logger.error(f"Insert failed for ObsID {problematic_obsid} in table {tbl}: {single_e}")
    
            self.conn.commit()
    
        finally:
            self._close_connection()


    def _map_data_type_to_sql(self, dtype):
        """
        Function to map the data types specified in get_keyword_types to 
        sqlite and psql datatypes.
        """
        if self.backend == 'sqlite':
            return {
                'int': 'INTEGER',
                'float': 'REAL',
                'bool': 'BOOLEAN',
                'datetime': 'TEXT',  # SQLite does not have a native datetime type
                'string': 'TEXT'
            }.get(dtype, 'TEXT')
        elif self.backend == 'psql':
            return {
                'int': 'INTEGER',
                'float': 'DOUBLE PRECISION',
                'bool': 'BOOLEAN',
                'datetime': 'TIMESTAMP',
                'string': 'TEXT'
            }.get(dtype, 'TEXT')
        else:
            raise ValueError(f"Unsupported backend: {self.backend}")


    def get_source(self, L0_dict):
        """
        Returns the name of the source in a spectrum.  For stellar observations, 
        this it returns 'Star'.  For calibration spectra, this is the lamp name 
        (ThAr, UNe, LFC, etalon) or bias/dark.  
        Flats using KPF's regular fibers are distinguished from wide flats.                           

        Returns:
            the source/image name
            possible values: 'Bias', 'Dark', 'Flat', 'Wide Flat', 
                             'LFC', 'Etalon', 'ThAr', 'UNe',
                             'Sun', 'Star'
        """
        try: 
            if (('ELAPSED' in L0_dict) and 
                ((L0_dict['IMTYPE'] == 'Bias') or (L0_dict['ELAPSED'] == 0))):
                    return 'Bias'
            elif L0_dict['IMTYPE'] == 'Dark':
                return 'Dark' 
            elif L0_dict['FFFB'].strip().lower() == 'yes':
                    return 'Wide Flat' # Flatfield Fiber (wide flats)
            elif L0_dict['IMTYPE'].strip().lower() == 'flatlamp':
                 if 'brdband' in L0_dict['OCTAGON'].strip().lower():
                    return 'Flat' # Flat through regular fibers
            elif L0_dict['IMTYPE'].strip().lower() == 'arclamp':
                if 'lfc' in L0_dict['OCTAGON'].strip().lower():
                    return 'LFC'
                if 'etalon' in L0_dict['OCTAGON'].strip().lower():
                    return 'Etalon'
                if 'th_' in L0_dict['OCTAGON'].strip().lower():
                    return 'ThAr'
                if 'u_' in L0_dict['OCTAGON'].strip().lower():
                    return 'UNe'
            elif ((L0_dict['TARGNAME'].strip().lower() == 'sun') or 
                  (L0_dict['TARGNAME'].strip().lower() == 'socal')):
                return 'Sun' # SoCal
            if ('OBJECT' in L0_dict) and ('FIUMODE' in L0_dict):
                if (L0_dict['FIUMODE'] == 'Observing'):
                    return 'Star'
        except:
            return 'Unknown'
        
    
    def _extract_kwd(self, file_path, keyword_types, extension='PRIMARY'):
        """
        Extracts specified header keywords from a given extension of a Keck 
        Planet Finder (KPF) FITS file.
    
        This method reads a FITS file, accesses the header of the designated 
        extension, and retrieves values for a specified list of keywords. 
        It also ensures special keywords such as `DRPTAG` and `DRPHASH`, 
        if present, propagate their values consistently across related data 
        levels (2D, L1, L2) by setting corresponding header keywords 
        (`DRPTAG2D`, `DRPHSH2D`, etc.).
    
        Workflow:
            1. Opens the specified FITS file at the given extension.
            2. Retrieves the requested keywords and initializes missing entries 
               as `None`.
            3. Ensures propagation of `DRPTAG` and `DRPHASH` values to 
               corresponding higher-level data keywords.
            4. Handles missing files or headers gracefully by returning a 
               dictionary filled with `None` values and logging an appropriate 
               error.
    
        Args:
            file_path (str):
                Path to the FITS file from which keywords are to be extracted.
            
            keyword_types (dict):
                Dictionary mapping keyword names to their intended data types 
                (e.g., 'float', 'string', 'bool'). Only keys from this 
                dictionary will be extracted from the header.
    
            extension (str, optional):
                FITS extension from which the header keywords are extracted. 
                Default is 'PRIMARY'.
    
        Returns:
            dict:
                A dictionary mapping each requested keyword to its extracted 
                value from the FITS header. If a keyword is missing from the 
                header, its value is set to `None`.
    
        Usage Scenario:
            - Typically utilized during data ingestion workflows to extract 
              relevant metadata from FITS file headers for database storage.
    
        Example:
            keyword_types = {'DATE-MID': 'datetime', 'TARGNAME': 'string', 'EXPTIME': 'float'}
            header_data = tsdb._extract_kwd('/data/L0/KP.20241001.12345.67.fits', keyword_types)
            # header_data = {
            #     'DATE-MID': '2024-10-01T12:34:56.789', 
            #     'TARGNAME': 'StarXYZ', 
            #     'EXPTIME': 300.0
            # }
        """
        # Initialize the result dictionary with None for all keywords
        header_data = {key: None for key in keyword_types.keys()}
    
        # Check if the file exists before proceeding
        if not os.path.isfile(file_path):
            return header_data
    
        try:
            # Open the FITS file and read the specified header
            with fits.open(file_path, memmap=True) as hdul:
                header = hdul[extension].header
    
                # Populate header_data from header
                header_data = {key: header.get(key, None) for key in keyword_types.keys()}
    
                # If DRPTAG is valid, propagate its value to appropriate data level
                drptag_value = header.get('DRPTAG', None)
                if drptag_value is not None:
                    for target_key in ['DRPTAG2D', 'DRPTAGL1', 'DRPTAGL2']:
                        if target_key in header_data:
                            header_data[target_key] = drptag_value
    
                # If DRPHASH is valid, propagate its value to appropriate data level
                drphash_value = header.get('DRPHASH', None)
                if drphash_value is not None:
                    for target_key in ['DRPHSH2D', 'DRPHSHL1', 'DRPHSHL2']:
                        if target_key in header_data:
                            header_data[target_key] = drphash_value
    
        except Exception as e:
            self.logger.error(f"Bad file: {file_path}. Error: {e}")
    
        return header_data


    def _extract_telemetry(self, file_path, keyword_types):
        """
        Extract telemetry data from the 'TELEMETRY' extension of a Keck Planet 
        Finder (KPF) Level 0 FITS file.
    
        This method reads telemetry keywords and their corresponding average 
        values stored within the FITS file's telemetry extension. It handles 
        data sanitization by converting invalid entries 
        (e.g., '-nan', 'nan', '-999') to numerical NaNs, ensuring consistency 
        for downstream database ingestion.
    
        Workflow:
            1. Opens the specified FITS file and accesses its 'TELEMETRY' 
               extension.
            2. Decodes and sanitizes telemetry keywords and their corresponding 
               average values.
            3. Constructs a dictionary mapping the requested telemetry keywords 
               to their sanitized numerical values.
    
        Args:
            file_path (str):
                Path to the KPF Level 0 FITS file containing telemetry data.
            keyword_types (dict):
                Dictionary with telemetry keywords as keys and their expected 
                data types (e.g., 'float') as values.
    
        Returns:
            dict:
                A dictionary mapping each requested telemetry keyword to its 
                corresponding numerical value.
                - Missing or invalid entries are returned as NaN.
                - If telemetry extraction fails due to file or format issues, 
                  returns a dictionary with all values set to None.

        Usage Scenario:
            - Typically invoked during observational data ingestion to populate 
              telemetry-specific database tables.
    
        Example:
            keyword_types = {'kpfmet.TEMP': 'float', 'kpfmet.PRESS': 'float'}
            telemetry_data = tsdb._extract_telemetry('/data/L0/KP.20241001.12345.67.fits', keyword_types)
            # telemetry_data = {'kpfmet.TEMP': 20.5, 'kpfmet.PRESS': 1.02}
        """
        try:
            # Use astropy's Table to load only necessary data
            telemetry_table = Table.read(file_path, format='fits', hdu='TELEMETRY')
            keywords = telemetry_table['keyword']
            averages = telemetry_table['average']
        except Exception as e:
            self.logger.info(f"Bad TELEMETRY extension in: {file_path}. Error: {e}")
            return {key: None for key in keyword_types}
    
        try:
            # Decode and sanitize 'keyword' column
            keywords = [k.decode('utf-8') if isinstance(k, bytes) else k for k in keywords]
    
            # Replace invalid values efficiently using NumPy
            averages = np.array(averages, dtype=object)  # Convert to object to allow mixed types
            mask_invalid = np.isin(averages, ['-nan', 'nan', -999]) | np.isnan(pd.to_numeric(averages, errors='coerce'))
            averages[mask_invalid] = np.nan
            averages = averages.astype(float)  # Convert valid data to float
    
            # Create the telemetry dictionary
            telemetry_data = dict(zip(keywords, averages))
    
            # Build the output dictionary for the requested keywords
            telemetry_dict = {key: float(telemetry_data.get(key, np.nan)) for key in keyword_types}
        except Exception as e:
            self.logger.info(f"Error processing TELEMETRY data in: {file_path}. Error: {e}")
            telemetry_dict = {key: None for key in keyword_types}
    
        return telemetry_dict


    def _extract_rvs(self, file_path):
        """
        Extract radial velocity (RV) measurements from the 'RV' extension of a 
        Keck Planet Finder (KPF) Level 2 FITS file.
    
        This method reads RV data structured across multiple spectral orders 
        from the designated RV extension. It maps specific RV-related 
        columnssuch as stellar RV, calibration RV, sky RV, RV errors, 
        barycentric corrections, and cross-correlation function weightsto 
        standardized database keyword names. Each extracted value is associated 
        with an order index (from 00 to 66).
    
        Workflow:
            1. Opens the Level 2 FITS file and accesses the 'RV' extension.
            2. Extracts RV-related columns including individual order RVs, 
               RV errors, barycentric RV corrections, and weights.
            3. Reorganizes extracted data into a standardized flat dictionary 
               format suitable for database ingestion.
            4. Ensures completeness and integrity of data, returning a 
               dictionary populated with None values if data extraction is 
               incomplete or erroneous.
    
        Args:
            file_path (str):
                Path to the KPF Level 2 FITS file containing radial velocity 
                data.
    
        Returns:
            dict:
                A dictionary with standardized keys mapping to radial velocity 
                measurements across all spectral orders (00 to 66). 
                Keys follow patterns such as:
                - 'RV1NN', 'RV2NN', 'RV3NN': Orderlet RVs for individual 
                  spectral orders.
                - 'RVSNN', 'ERVSNN': Stellar RV and corresponding error.
                - 'RVCNN', 'ERVCNN': Calibration RV and error.
                - 'RVYNN', 'ERVYNN': Sky RV and error.
                - 'CCFBJDNN': Barycentric Julian Date for each order.
                - 'BCRVNN': Barycentric radial velocity correction.
                - 'CCFWNN': Cross-correlation function weights.
                
                Example key format: 'RVS01' represents stellar RV for order 01.
    
                If data extraction fails or incomplete data is detected, all 
                values are returned as None.
    
        Error Handling:
            - Logs an error message and returns a fully populated dictionary of 
              None values if file reading or data parsing fails, ensuring 
              database integrity.
    
        Usage Scenario:
            - Used primarily during data ingestion processes for populating the 
              RV-specific database tables.
    
        Example:
            rv_data = tsdb._extract_rvs('/data/L2/KP.20241001.12345.67_L2.fits')
            # rv_data = {
            #     'RVS00': -12.345, 'ERVS00': 0.005, 'RVC00': -12.350, 'ERVC00': 0.004, ..., 'CCFW66': 0.95
            # }
        """
        mapping = {
            'orderlet1':   'RV1{}',
            'orderlet2':   'RV2{}',
            'orderlet3':   'RV3{}',
            'RV':          'RVS{}',
            'RV error':    'ERVS{}',
            'CAL RV':      'RVC{}',
            'CAL error':   'ERVC{}',
            'SKY RV':      'RVY{}',
            'SKY error':   'ERVY{}',
            'CCFBJD':      'CCFBJD{}',
            'Bary_RVC':    'BCRV{}',
            'CCF Weights': 'CCFW{}',
        }
    
        cols = ['orderlet1', 'orderlet2', 'orderlet3', 'RV', 'RV error', 
                'CAL RV', 'CAL error', 'SKY RV', 'SKY error', 'CCFBJD', 
                'Bary_RVC', 'CCF Weights']
    
        expected_count = 67 * len(cols)
    
        def make_dummy_dict():
            keys = []
            for i in range(0, 67):
                NN = f"{i:02d}"  # two-digit row number, from 00 to 66
                for pattern in mapping.values():
                    keys.append(pattern.format(NN))
            return {key: None for key in keys}
        
        try:
            df_rv = Table.read(file_path, format='fits', hdu='RV').to_pandas()
            df_rv = df_rv[cols]
        except Exception as e:
            # If we can't read RVs, return a dict with None values for all expected keys
            rv_dict = make_dummy_dict()
            return rv_dict
    
        df_filtered = df_rv[list(mapping.keys())]
        stacked = df_filtered.stack()
        keyed = stacked.reset_index()
        keyed.columns = ['row_idx', 'col', 'val']
        keyed['NN'] = keyed['row_idx'].apply(lambda x: f"{x:02d}")  # zero-based indexing
        keyed['key'] = keyed['col'].map(mapping)
        keyed['key'] = keyed['key'].str[:-2] + keyed['NN']
        rv_dict = dict(zip(keyed['key'], keyed['val']))
    
        # Check the count - if data wasn't computed for Green or Red, the database will give an error on insertion
        if len(rv_dict) != expected_count:
            # If count doesn't match, return the dummy dictionary of None values
            rv_dict = make_dummy_dict()
            
        return rv_dict        


    def clean_df(self, df):
        """
        Remove known outliers from a dataframe.
        """
        # CCD Read Noise
        cols = ['RNGREEN1', 'RNGREEN2', 'RNGREEN3', 'RNGREEN4', 'RNRED1', 'RNRED2', 'RNRED3', 'RNRED4']
        for col in cols:
            if col in df.columns:
                df = df.loc[df[col] < 500]
        
        # Hallway temperature
        if 'kpfmet.TEMP' in df.columns:
            df = df.loc[df['kpfmet.TEMP'] > 15]
        
        # Fiber temperatures
        kwrds = ['kpfmet.SIMCAL_FIBER_STG', 'kpfmet.SIMCAL_FIBER_STG']
        for key in kwrds:
            if key in df.columns:
                df = df.loc[df[key] > 0]
                       
        # Dark Current
        kwrds = ['FLXCOLLG', 'FLXECHG', 'FLXREG1G', 'FLXREG2G', 'FLXREG3G', 'FLXREG4G', 
                 'FLXREG5G', 'FLXREG6G', 'FLXCOLLR', 'FLXECHR', 'FLXREG1R', 'FLXREG2R', 
                 'FLXREG3R', 'FLXREG4R', 'FLXREG5R', 'FLXREG6R']

        # Spectrometer and other temperatures from kpfmet
        kwrds = ['kpfmet.BENCH_BOTTOM_BETWEEN_CAMERAS', 'kpfmet.BENCH_BOTTOM_COLLIMATOR', 'kpfmet.BENCH_BOTTOM_DCUT', 'kpfmet.BENCH_BOTTOM_ECHELLE', 'kpfmet.BENCH_TOP_BETWEEN_CAMERAS', 'kpfmet.BENCH_TOP_COLL', 'kpfmet.BENCH_TOP_DCUT', 'kpfmet.BENCH_TOP_ECHELLE_CAM', 'kpfmet.CALEM_SCMBLR_CHMBR_END', 'kpfmet.CALEM_SCMBLR_FIBER_END', 'kpfmet.CAL_BENCH', 'kpfmet.CAL_BENCH_BB_SRC', 'kpfmet.CAL_BENCH_BOT', 'kpfmet.CAL_BENCH_ENCL_AIR', 'kpfmet.CAL_BENCH_OCT_MOT', 'kpfmet.CAL_BENCH_TRANS_STG_MOT', 'kpfmet.CAL_RACK_TOP|float', 'kpfmet.CHAMBER_EXT_BOTTOM', 'kpfmet.CHAMBER_EXT_TOP', 'kpfmet.CRYOSTAT_G1', 'kpfmet.CRYOSTAT_G2', 'kpfmet.CRYOSTAT_G3', 'kpfmet.CRYOSTAT_R1', 'kpfmet.CRYOSTAT_R2', 'kpfmet.CRYOSTAT_R3', 'kpfmet.ECHELLE_BOTTOM', 'kpfmet.ECHELLE_TOP', 'kpfmet.FF_SRC', 'kpfmet.GREEN_CAMERA_BOTTOM', 'kpfmet.GREEN_CAMERA_COLLIMATOR', 'kpfmet.GREEN_CAMERA_ECHELLE', 'kpfmet.GREEN_CAMERA_TOP', 'kpfmet.GREEN_GRISM_TOP', 'kpfmet.GREEN_LN2_FLANGE', 'kpfmet.PRIMARY_COLLIMATOR_TOP', 'kpfmet.RED_CAMERA_BOTTOM', 'kpfmet.RED_CAMERA_COLLIMATOR', 'kpfmet.RED_CAMERA_ECHELLE', 'kpfmet.RED_CAMERA_TOP', 'kpfmet.RED_GRISM_TOP', 'kpfmet.RED_LN2_FLANGE', 'kpfmet.REFORMATTER', 'kpfmet.SCIENCE_CAL_FIBER_STG', 'kpfmet.SCISKY_SCMBLR_CHMBR_EN', 'kpfmet.SCISKY_SCMBLR_FIBER_EN', 'kpfmet.SIMCAL_FIBER_STG', 'kpfmet.SKYCAL_FIBER_STG', 'kpfmet.TEMP', 'kpfmet.TH_DAILY', 'kpfmet.TH_GOLD', 'kpfmet.U_DAILY', 'kpfmet.U_GOLD',]
        for key in kwrds:
            if key in df.columns:
                pass
                # need to check on why these values occur
                #df = df.loc[df[key] > -200]
                #df = df.loc[df[key] <  400]

        return df


    @require_role(['admin', 'operations', 'readonly'])
    def _is_any_file_updated(self, L0_file_path):
        """
        Determines if any file from the L0/2D/L1/L2 set has been updated since 
        the last noted modification in the database. 
        Returns True if any file has been modified.
        """
        L0_filename = os.path.basename(L0_file_path)
    
        timestamp_columns = ["L0_header_read_time", "D2_header_read_time", 
                             "L1_header_read_time", "L2_header_read_time"]
    
        if self.backend == 'psql':
            # Quote columns explicitly for PostgreSQL
            col_str = ', '.join(f'"{col}"' for col in timestamp_columns)
            filename_col = '"L0_filename"'
            placeholder = '%s'
        else:
            # SQLite does not need explicit quoting here
            col_str = ', '.join(timestamp_columns)
            filename_col = 'L0_filename'
            placeholder = '?'
    
        query = f'SELECT {col_str} FROM tsdb_base WHERE {filename_col} = {placeholder}'
    
        self._open_connection()
        try:
            result = self._execute_sql_command(query, params=(L0_filename,), fetch=True)
        finally:
            self._close_connection()
    
        if not result:
            return True
    
        result = result[0]
    
        file_paths = {
            'L0': L0_file_path,
            'D2': L0_file_path.replace('L0', '2D').replace('.fits', '_2D.fits'),
            'L1': L0_file_path.replace('L0', 'L1').replace('.fits', '_L1.fits'),
            'L2': L0_file_path.replace('L0', 'L2').replace('.fits', '_L2.fits'),
        }
    
        for idx, (key, path) in enumerate(file_paths.items()):
            try:
                mod_time = datetime.fromtimestamp(os.path.getmtime(path)).strftime("%Y-%m-%d %H:%M:%S")
            except FileNotFoundError:
                mod_time = '1000-01-01 00:00:00'
    
            if mod_time > (result[idx] or '1000-01-01 00:00:00'):
                return True
    
        return False


    @require_role(['admin', 'operations'])
    def add_ObsID_list_to_db(self, ObsID_filename, reverse=False):
        """
        Read a CSV file with ObsID values in the first column and ingest those 
        files into the database.  If reverse=True, then they will be ingested 
        in reverse chronological order.
        """
        if os.path.isfile(ObsID_filename):
            try:
                df = pd.read_csv(ObsID_filename)
            except Exception as e:
                self.logger.info(f'Problem reading {ObsID_filename}: ' + e)
        else:
            self.logger.info('File missing: ObsID_filename')
        
        ObsID_pattern = r'KP\.20\d{6}\.\d{5}\.\d{2}'
        first_column = df.iloc[:, 0]
        filtered_column = first_column[first_column.str.match(ObsID_pattern)]
        df = filtered_column.to_frame()
        column_name = df.columns[0]
        df.rename(columns={column_name: 'ObsID'}, inplace=True)
        if reverse:
            df = df.sort_values(by='ObsID', ascending=False)
        else:
            df = df.sort_values(by='ObsID', ascending=True)

        self.logger.info(f'{ObsID_filename} read with {str(len(df))} properly formatted ObsIDs.')

        t = self.tqdm(df.iloc[:, 0].tolist(), desc=f'ObsIDs', leave=True)
        for ObsID in t:
            dir_path = self.base_dir + '/' + get_datecode(ObsID) + '/'
            filename = ObsID + '.fits'
            file_path = os.path.join(dir_path, filename)
            base_filename = filename.split('.fits')[0]
            t.set_description(base_filename)
            t.refresh() 
            try:
                if os.path.exists(ObsID_filename):
                    self.ingest_one_observation(dir_path, filename) 
            except Exception as e:
                self.logger.error(e)


    @require_role(['admin', 'operations'])
    def add_ObsIDs_to_db(self, ObsID_list):
        """
        Ingest files into the database from a list of strings 'ObsID_list'.  
        """
        t = self.tqdm(ObsID_list, desc=f'ObsIDs', leave=True)
        for ObsID in t:
            L0_filename = ObsID + '.fits'
            dir_path = self.base_dir + '/' + get_datecode(ObsID) + '/'
            file_path = os.path.join(dir_path, L0_filename)
            base_filename = L0_filename.split('.fits')[0]
            t.set_description(base_filename)
            t.refresh() 
            try:
                self.ingest_one_observation(dir_path, L0_filename) 
            except Exception as e:
                self.logger.error(e)


    @require_role(['admin', 'operations', 'readonly'])
    def print_metadata_table(self):
        """
        Read the tsdb_metadata table, group by 'source', and print out rows
        in fixed-width columns in the custom order below, including the 
        'indexed' column. If units=NaN, it prints a blank.
        """
        custom_order = [
            "Base Keywords",
            "L0 PRIMARY Header",
            "2D PRIMARY Header",
            "L1 PRIMARY Header",
            "L2 PRIMARY Header",
            "L0 TELEMETRY Extension",
            "L2 RV Header",
            "L2 RV Extension",
            "L2 CCF Header"
        ]
    
        col_width_keyword = 35
        col_width_datatype = 9
        col_width_indexed = 7
        col_width_units = 9
        col_width_desc = 68
        col_width_dashes = col_width_keyword + col_width_datatype + col_width_indexed + col_width_units + col_width_desc
    
        self._open_connection()
        try:
            for src in custom_order:
                query = """
                    SELECT keyword, datatype, indexed, units, description
                    FROM tsdb_metadata
                    WHERE source = ?
                    ORDER BY keyword;
                """
                self._execute_sql_command(query, params=(src,))
                rows = self.cursor.fetchall()
    
                if not rows:
                    continue
    
                print(f"{src}:")
                print("-" * col_width_dashes)
                print(
                    f"{'Keyword':<{col_width_keyword}} "
                    f"{'Datatype':<{col_width_datatype}} "
                    f"{'Indexed':<{col_width_indexed}} "
                    f"{'Units':<{col_width_units}} "
                    f"{'Description':<{col_width_desc}}"
                )
                print("-" * col_width_dashes)
    
                for keyword, datatype, indexed, units, description in rows:
                    units_str = "" if units in (None, "NaN", "nan", float("nan")) or pd.isna(units) else units
                    indexed_str = 'Yes' if indexed else 'No'
    
                    print(
                        f"{keyword:<{col_width_keyword}} "
                        f"{datatype:<{col_width_datatype}} "
                        f"{indexed_str:<{col_width_indexed}} "
                        f"{units_str:<{col_width_units}} "
                        f"{description:<{col_width_desc}}"
                    )
                print()
        finally:
            self._close_connection()


    @require_role(['admin', 'operations', 'readonly'])
    def metadata_table_to_df(self):
        """
        Return a dataframe of the metadata table with columns:
            keyword | datatype | units | description | source
        """
        self._open_connection()
        try:
            query = "SELECT keyword, datatype, units, description, source FROM tsdb_metadata"
            self._execute_sql_command(query)
            rows = self.cursor.fetchall()
    
            df = pd.DataFrame(rows, columns=['keyword', 'datatype', 'units', 'description', 'source'])
        finally:
            self._close_connection()
    
        return df


    @require_role(['admin', 'operations', 'readonly'])
    def print_table_contents(self, table_name):
        """
        Print the contents of the specified table. This is useful for debugging.
    
        Args:
            table_name (str): Name of the table to print.
        """
        self._open_connection()
        try:
            # Fetch all data from the table
            self._execute_sql_command(f"SELECT * FROM {table_name}")
            rows = self.cursor.fetchall()
    
            # Fetch column names
            column_names = [description[0] for description in self.cursor.description]
    
            # Print column names
            print(f"Contents of table '{table_name}':")
            print("-" * 100)
            print('\t'.join(column_names))
            print("-" * 100)
    
            # Print rows
            for row in rows:
                print('\t'.join([str(item) if item is not None else '' for item in row]))
                
            print("-" * 100)
            print(f"Total rows: {len(rows)}")
    
        finally:
            self._close_connection()


    def is_notebook(self):
        """
        Determine if the code is being executed in a Jupyter Notebook.  
        This is useful for tqdm.
        """
        try:
            from IPython import get_ipython
            if 'IPKernelApp' not in get_ipython().config:  # Notebook not running
                return False
        except (ImportError, AttributeError):
            return False  # IPython not installed
        return True


    @require_role(['admin', 'operations', 'readonly'])
    def get_first_last_dates(self):
        """
        Returns a tuple of datetime objects containing the first and last dates 
        in the database. DATE-MID is used for the date.
        """
        self._open_connection()
        try:
            query = """
                SELECT MIN("DATE-MID") AS min_date, MAX("DATE-MID") AS max_date
                FROM tsdb_l0
            """
            self._execute_sql_command(query)
            min_date_raw, max_date_raw = self.cursor.fetchone()
    
            date_format = '%Y-%m-%dT%H:%M:%S.%f'
    
            def parse_date(date_input):
                if isinstance(date_input, datetime):
                    return date_input
                elif isinstance(date_input, str):
                    return datetime.strptime(date_input, date_format)
                else:
                    return None
    
            first_date = parse_date(min_date_raw)
            last_date = parse_date(max_date_raw)
    
        finally:
            self._close_connection()
    
        return first_date, last_date


    @require_role(['admin', 'operations', 'readonly'])
    def ObsIDlist_from_db(self, object_name, start_date=None, end_date=None, not_junk=None):
        """
        Returns a list of ObsIDs for the observations of object_name.

        Args:
            object_name (string) - name of object (e.g., '4614')
            not_junk (True, False, None) using NOTJUNK, select observations that 
                are not Junk (True), Junk (False), or don't care (None)
            start_date (datetime object) - only return observations after start_date
            end_date (datetime object) - only return observations before end_date

        Returns:
            Pandas dataframe of the specified columns matching the constraints.
        """
        df = self.dataframe_from_db(columns=['ObsID'], object_like=object_name, 
                                    start_date=start_date, end_date=end_date, 
                                    not_junk=not_junk)
        
        return df['ObsID'].tolist()


    @require_role(['admin', 'operations', 'readonly'])
    def dataframe_from_db(self, columns=None, 
                          start_date=None, end_date=None, 
                          only_object=None, only_source=None, object_like=None, 
                          on_sky=None, not_junk=None, 
                          QC_pass=None, QC_fail=None, 
                          QC_not_pass=None, QC_not_fail=None, 
                          extra_conditions=None,
                          extra_conditions_logic='AND',
                          verbose=False):
        """
        Description:
            Return a Pandas DataFrame containing specified columns from a 
            joined set of database tables, applying optional filters based on 
            object names, source types, date ranges, sky condition, and quality 
            checks.
    
        Args:
            columns (str or list of str, optional): Column name(s) to retrieve. 
                Defaults to None (fetches all columns).
            start_date (str or datetime, optional): Starting date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            end_date (str or datetime, optional): Ending date for filtering 
                observations (datetime object or YYYYMMDD or None). Defaults to 
                None.
            only_object (str or list of str, optional): Exact object name(s) to 
                filter observations. Defaults to None.
                E.g., only_object = ['autocal-dark', 'autocal-bias']
            only_source (str or list of str, optional): Source type(s) to filter 
                observations. Defaults to None.
                E.g., only_source = ['Dark', 'Bias']
            object_like (str or list of str, optional): Partial object name(s) 
                for filtering observations using SQL LIKE conditions. Defaults 
                to None.
                E.g., object_like = ['autocal-etalon', 'autocal-bias']
            on_sky (bool, optional): Filter by on-sky (True) or calibration 
                (False) observations. Defaults to None.
            QC_pass (str or list of str, optional): Column names where rows 
                must have True. Defaults to None.
            QC_fail (str or list of str, optional): Column names where rows 
                must have False. Defaults to None.
            QC_not_pass (str or list of str, optional): Column names where rows 
                are not True; allowable values are False and Null. A Null entry 
                can happen when an observation is ingested for which a QC test 
                has not been performed (e.g., because the QC was recently 
                developed). Defaults to None.
            QC_not_fail (str or list of str, optional): Column names where rows 
                are not False; allowable values are True and Null. A Null entry 
                can happen when an observation is ingested for which a QC test 
                has not been performed (e.g., because the QC was recently 
                developed). Defaults to None.
            not_junk (bool, optional): Filter by observations marked as not junk 
                (True) or junk (False). Defaults to None.
            verbose (bool, optional): Enables detailed logging of SQL queries 
                and parameters. Defaults to False.
    
        Returns:
            The resulting dataframe.

        """
    
        if isinstance(only_object, str):
            only_object = [only_object]
        if isinstance(only_source, str):
            only_source = [only_source]
        if isinstance(object_like, str):
            object_like = [object_like]
        if isinstance(QC_pass, str):
            QC_pass = [QC_pass]
        if isinstance(QC_fail, str):
            QC_fail = [QC_fail]
        if isinstance(QC_not_pass, str):
            QC_not_pass = [QC_not_pass]
        if isinstance(QC_not_fail, str):
            QC_not_fail = [QC_not_fail]
    
        quote = '"' if self.backend == 'psql' else '"'  # SQLite uses " for quoting as well
        placeholder = '%s' if self.backend == 'psql' else '?'
    
        self._open_connection()
    
        try:
            # Get column metadata
            if columns in (None, '*'):
                metadata_query = 'SELECT keyword, table_name FROM tsdb_metadata;'
                self._execute_sql_command(metadata_query)
                metadata = pd.DataFrame(self.cursor.fetchall(), columns=['keyword', 'table_name'])
                columns_requested = metadata['keyword'].tolist()
            else:
                columns_requested = [columns] if isinstance(columns, str) else columns
                columns_needed = columns_requested.copy()
                if QC_pass is not None:
                    columns_needed.extend(QC_pass)
                if QC_fail is not None:
                    columns_needed.extend(QC_fail)
                if QC_not_pass is not None:
                    columns_needed.extend(QC_not_pass)
                if QC_not_fail is not None:
                    columns_needed.extend(QC_not_fail)
                placeholders = ','.join([placeholder] * len(columns_needed))
                metadata_query = f'SELECT keyword, table_name FROM tsdb_metadata WHERE keyword IN ({placeholders});'
                self._execute_sql_command(metadata_query, params=columns_needed)
                metadata = pd.DataFrame(self.cursor.fetchall(), columns=['keyword', 'table_name'])
    
            kw_table_map = dict(zip(metadata['keyword'], metadata['table_name']))
            tables_needed = set(metadata['table_name'].dropna())
            tables_needed.update(['tsdb_base', 'tsdb_l0', 'tsdb_2d'])
    
            # Prepare select columns with proper quoting
            table_columns = {table: {'ObsID'} for table in tables_needed}
            for kw, tbl in kw_table_map.items():
                table_columns[tbl].add(kw)

            guaranteed_columns = {
                'tsdb_base': ['Source', 'datecode'],
                'tsdb_l0': ['OBJECT', 'FIUMODE'],
                'tsdb_2d': ['NOTJUNK']
            }
            for tbl, cols in guaranteed_columns.items():
                table_columns[tbl].update(cols)
    
            select_clauses = []
            selected_cols_set = set()
            for tbl in tables_needed:
                for col in table_columns[tbl]:
                    col_quoted = f'{quote}{col}{quote}'
                    alias_quoted = f'{quote}{col}{quote}'
                    if col == 'ObsID' and col in selected_cols_set:
                        continue
                    selected_cols_set.add(col)
                    select_clauses.append(f'{tbl}.{col_quoted} AS {alias_quoted}')
    
            select_sql = ', '.join(select_clauses)
    
            from_clause = 'tsdb_base'
            join_clauses = [
                f'LEFT JOIN {tbl} ON tsdb_base.{quote}ObsID{quote} = {tbl}.{quote}ObsID{quote}'
                for tbl in tables_needed if tbl != 'tsdb_base'
            ]

            conditions, params = [], []
    
            if only_object:
                placeholders = ','.join([placeholder] * len(only_object))
                conditions.append(f'tsdb_l0.{quote}OBJECT{quote} IN ({placeholders})')
                params.extend(only_object)
    
            if object_like:
                like_conditions = [f'tsdb_l0.{quote}OBJECT{quote} LIKE {placeholder}' for _ in object_like]
                conditions.append('(' + ' OR '.join(like_conditions) + ')')
                params.extend([f'%{ol}%' for ol in object_like])
    
            if only_source:
                placeholders = ','.join([placeholder] * len(only_source))
                conditions.append(f'tsdb_base.{quote}Source{quote} IN ({placeholders})')
                params.extend(only_source)
    
            if not_junk is not None:
                conditions.append(f'tsdb_2d.{quote}NOTJUNK{quote} = {placeholder}')
                params.append(True if not_junk else False)
    
            if on_sky is not None:
                mode = 'Observing' if on_sky else 'Calibration'
                conditions.append(f'tsdb_l0.{quote}FIUMODE{quote} = {placeholder}')
                params.append(mode)

            if QC_pass is not None:
                for col in QC_pass:
                    conditions.append(f"{quote}{col}{quote} = {placeholder}")
                    params.append(True)
        
            if QC_fail is not None:
                for col in QC_fail:
                    conditions.append(f"{quote}{col}{quote} = {placeholder}")
                    params.append(False)
    
            if QC_not_pass is not None:
                for col in QC_not_pass:
                    conditions.append(f"({quote}{col}{quote} IS NULL OR {quote}{col}{quote} = {placeholder})")
                    params.append(False)
        
            if QC_not_fail is not None:
                for col in QC_not_pass:
                    conditions.append(f"({quote}{col}{quote} IS NULL OR {quote}{col}{quote} = {placeholder})")
                    params.append(False)
    
            if start_date:
                date_str = pd.to_datetime(start_date).strftime("%Y%m%d")
                conditions.append(f'tsdb_base.{quote}datecode{quote} >= {placeholder}')
                params.append(date_str)
    
            if end_date:
                date_str = pd.to_datetime(end_date).strftime("%Y%m%d")
                conditions.append(f'tsdb_base.{quote}datecode{quote} <= {placeholder}')
                params.append(date_str)
    
            if extra_conditions:
                qualified_extra_conditions = []
                for condition in extra_conditions:
                    for keyword, table in kw_table_map.items():
                        condition = re.sub(rf'\\b{keyword}\\b', f'{table}.{quote}{keyword}{quote}', condition)
                    qualified_extra_conditions.append(condition)
    
                extra_clause = f" {extra_conditions_logic} ".join(qualified_extra_conditions)
                conditions.append(f"({extra_clause})")
            
            where_clause = f"WHERE {' AND '.join(conditions)}" if conditions else ""
    
            query = f"""
                SELECT {select_sql}
                FROM {from_clause}
                {' '.join(join_clauses)}
                {where_clause}
            """

            if verbose:
                self.logger.debug("SQL Query:")
                self.logger.debug(query)
                self.logger.debug("Params:")
                self.logger.debug(params)
    
            self._execute_sql_command(query, params)
            fetched_data = self.cursor.fetchall()
            col_names = [desc[0] for desc in self.cursor.description]
    
            df = pd.DataFrame(fetched_data, columns=col_names)
    
            # Reorder columns if explicitly requested
            if columns not in (None, '*'):
                final_column_order = [col for col in columns_requested if col in df.columns]
                df = df.sort_values(by='ObsID', ascending=True)
                if 'ObsID' in df.columns and 'ObsID' not in columns_requested:
                    df = df.drop(columns='ObsID')
                df = df[final_column_order]
    
        finally:
            self._close_connection()
    
        return df


    @require_role(['admin', 'operations', 'readonly'])
    def display_data(self, df=None, columns=None, 
                           start_date=None, end_date=None, 
                           only_object=None, object_like=None, only_source=None, 
                           on_sky=None, not_junk=None,
                           QC_pass=None, QC_fail=None, 
                           QC_not_pass=None, QC_not_fail=None, 
                           max_height_px=600, # in pixels
                           url_stub='https://jump.caltech.edu/observing-logs/kpf/',
                           verbose=False):
        """
        Description:
            Make a formatted printout of a pandas DataFrame containing specified 
            columns from a joined set of database tables, applying optional 
            filters based on object names, source types, date ranges, 
            sky condition, and quality checks. The table includes ObsIDs links, 
            enables scrolling if necessary, and makes columns sortable.

        Args:
            df (dataframe, default=None): If df == None, generate a dataframe, 
                otherwise use in the input df
            max_height_px (int, default=800): Sets the vertical height (pixels) 
                for scrolling.
            url_stub (str): the URL for ObsID links.  The default page is set 
                to "Jump", the portal used by the KPF Science Team.
            (other arguments are the same as dataframe_from_db)

        Returns:
            None. Prints the resulting dataframe.
        """
        
        if df is None:
            df = self.dataframe_from_db(
                columns=columns,
                only_object=only_object,
                object_like=object_like,
                only_source=only_source,
                on_sky=on_sky, 
                not_junk=not_junk,
                QC_pass=QC_pass, 
                QC_fail=QC_fail, 
                QC_not_pass=QC_not_pass, 
                QC_not_fail=QC_not_fail, 
                start_date=start_date,
                end_date=end_date,
                verbose=verbose
            )

        # Convert ObsID to clickable HTML links
        if 'ObsID' in df.columns:
            df['ObsID'] = df['ObsID'].apply(
                lambda obsid: f'<a href="{url_stub}{obsid}" target="_blank">{obsid}</a>'
            )

        # Generate HTML table with scrolling and sortable columns
        html = df.to_html(escape=False, index=False, classes='sortable')

        # Wrap in styled div and JavaScript for sorting
        styled_html = f'''
        <style>
        .sortable th {{
            cursor: pointer;
            background-color: #f1f1f1;
        }}
        </style>
        <div style="overflow:auto; max-height:{max_height_px}px;">
            {html}
        </div>
        <script>
        document.addEventListener('DOMContentLoaded', function() {{
            function sortTable(table, colIndex, asc) {{
                const tbody = table.tBodies[0];
                const rows = Array.from(tbody.querySelectorAll('tr'));

                rows.sort((a, b) => {{
                    const aText = a.children[colIndex].innerText;
                    const bText = b.children[colIndex].innerText;
                    const aNum = parseFloat(aText);
                    const bNum = parseFloat(bText);
                    if (!isNaN(aNum) && !isNaN(bNum)) {{
                        return asc ? aNum - bNum : bNum - aNum;
                    }} else {{
                        return asc ? aText.localeCompare(bText) : bText.localeCompare(aText);
                    }}
                }});

                rows.forEach(row => tbody.appendChild(row));
            }}

            document.querySelectorAll('table.sortable th').forEach((th, index) => {{
                let ascending = true;
                th.addEventListener('click', () => {{
                    const table = th.closest('table');
                    sortTable(table, index, ascending);
                    ascending = !ascending;
                }});
            }});
        }});
        </script>
        '''

        # Display the formatted table
        display(HTML(styled_html))


    def print_log_error_report(self, df, log_dir='/data/logs/', aggregated_summary=False):
        '''
        For each ObsID in the dataframe, open the corresponding log file,
        find all lines containing [ERROR]:, and print either:
        - aggregated error report (if aggregated_summary=True)
        - individual ObsID error reports (if aggregated_summary=False)
        '''
        error_counter = Counter()  # Collect error bodies for aggregation
    
        for obsid in df['ObsID']:
            log_path = os.path.join(log_dir, f'{get_datecode(obsid)}/{obsid}.log')
            
            if not os.path.isfile(log_path):
                if not aggregated_summary:
                    print(f"ObsID: {obsid}")
                    print(f"Log file: {log_path}")
                    print(f"Log file not found.\n")
                continue
            
            mod_time = datetime.utcfromtimestamp(os.path.getmtime(log_path)).strftime('%Y-%m-%d %H:%M:%S UTC')
            
            error_lines = []
            with open(log_path, 'r') as file:
                for line in file:
                    if '[ERROR]:' in line:
                        error_line = line.strip()
                        error_lines.append(error_line)
    
                        # Extract only the part after [ERROR]:
                        parts = error_line.split('[ERROR]:', 1)
                        if len(parts) > 1:
                            error_body = parts[1].strip()
                            error_counter[error_body] += 1
                        else:
                            error_counter[error_line] += 1
            
            if not aggregated_summary:
                # Print individual ObsID report
                print(f"ObsID: {obsid}")
                print(f"Log file: {log_path}")
                print(f"Log modification date: {mod_time}")
                print(f"Errors in log file:")
                
                if error_lines:
                    for error in error_lines:
                        print(f"    {error}")
                else:
                    print(f"    No [ERROR] lines found.")
                
                print("\n" + "-" * 50 + "\n")
        
        # After processing all ObsIDs, print the aggregated summary if requested
        if aggregated_summary:
            if error_counter:
                print("\nAggregated Error Summary:\n")
                
                summary_df = pd.DataFrame(
                    [(count, error) for error, count in error_counter.items()],
                    columns=['Count', 'Error Message']
                ).sort_values('Count', ascending=False).reset_index(drop=True)
                
                # Set wide display options for Pandas
                pd.set_option('display.max_colwidth', None)
                pd.set_option('display.width', 0)
                
                # Force all table cells to left-align with inline CSS
                html = summary_df.to_html(index=False, escape=False)
                html = html.replace('<td>', '<td style="text-align: left; white-space: normal; word-wrap: break-word;">')
                html = html.replace('<th>', '<th style="text-align: left; white-space: normal; word-wrap: break-word;">')
                
                display(HTML(html))
            else:
                print("No [ERROR] lines found across all logs.")


def process_file(file_path, now_str,
                 extraction_plan, bool_columns, kw_to_table, keywords_by_table, kw_to_dtype,
                 _extract_kwd_func, _extract_telemetry_func, _extract_rvs_func,
                 get_source_func, get_datecode_func, safe_float):
    """
    Process a single file to extract all relevant keywords based on the 
    extraction plan.
    """    
    
    base_filename = os.path.basename(file_path).replace('.fits', '')
    file_level_paths = {
        'L0': file_path,
        '2D': file_path.replace('L0', '2D').replace('.fits', '_2D.fits'),
        'L1': file_path.replace('L0', 'L1').replace('.fits', '_L1.fits'),
        'L2': file_path.replace('L0', 'L2').replace('.fits', '_L2.fits'),
    }

    extraction_results = {}

    for tbl, plan in extraction_plan.items():
        file_level = plan['file_level']
        extension = plan['extension']
        current_file_path = file_level_paths[file_level]

        keywords = keywords_by_table.get(tbl, [])
        kw_types = {kw: kw_to_dtype[kw] for kw in keywords}

        if not os.path.exists(current_file_path):
            continue

        if tbl == 'tsdb_l0t':
            extracted = _extract_telemetry_func(current_file_path, kw_types)
        elif tbl.startswith('tsdb_l2_') and tbl not in ['tsdb_l2', 'tsdb_l2rv', 'tsdb_l2ccf']:
            extracted = _extract_rvs_func(current_file_path)
        else:
            extracted = _extract_kwd_func(current_file_path, kw_types, extension)

        extraction_results.update(extracted)

    # the Angle code was crashing
    def safe_angle(value, unit, default=0.0):
        try:
            return Angle(value, unit=unit).degree
        except Exception:
            return default

    # Mandatory metadata
    extraction_results.update({
        'ObsID': base_filename,
        'datecode': get_datecode_func(base_filename),
        'Source': get_source_func(extraction_results),
        'L0_filename': os.path.basename(file_level_paths['L0']),
        'D2_filename': os.path.basename(file_level_paths['2D']),
        'L1_filename': os.path.basename(file_level_paths['L1']),
        'L2_filename': os.path.basename(file_level_paths['L2']),
        'L0_header_read_time': now_str,
        'D2_header_read_time': now_str,
        'L1_header_read_time': now_str,
        'L2_header_read_time': now_str,
        'RA_deg': safe_angle(extraction_results.get('RA'), unit='hourangle'),
        'DEC_deg': safe_angle(extraction_results.get('DEC'), unit='deg')
    })

    # Convert boolean columns explicitly
    for col in bool_columns:
        if col in extraction_results:
            val = extraction_results[col]
            extraction_results[col] = bool(val) if isinstance(val, (bool, int)) else str(val).strip().lower() in ['1', 'true', 't', 'yes', 'y']

    # Apply safe_float explicitly to all float columns
    for kw, dtype in kw_to_dtype.items():
        if dtype == 'float' and kw in extraction_results:
            extraction_results[kw] = safe_float(extraction_results[kw])

    return extraction_results


def convert_to_list_if_array(string):
    """
    Convert a string like '["autocal-lfc-all-morn", "autocal-lfc-all-eve"]' to an array.
    """
    # Check if the string starts with '[' and ends with ']'
    if type(string) == 'str':
        if string.startswith('[') and string.endswith(']'):
            try:
                # Attempt to parse the string as JSON
                return json.loads(string)
            except json.JSONDecodeError:
                # The string is not a valid JSON array
                return string
    else:
        # The string does not look like a JSON array
        return string
